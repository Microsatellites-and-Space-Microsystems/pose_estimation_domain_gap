{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CNN-ViT Training\n",
        "This Notebook illustrates how to build and train a CNN-ViT model. The notebook is configured for running on a TPU hosted runtime on Google Colab."
      ],
      "metadata": {
        "id": "uuOss4gQBrh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminaries"
      ],
      "metadata": {
        "id": "KHdhJKxqr-k3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required packages."
      ],
      "metadata": {
        "id": "bH1dn8iEJ9SN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/Microsatellites-and-Space-Microsystems/pose_estimation_domain_gap --quiet"
      ],
      "metadata": {
        "id": "rdKMtwSdJ8_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide access to Google Drive."
      ],
      "metadata": {
        "id": "zeY_HYoGsFLQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WENENa4FMv_Q"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set network name and directories."
      ],
      "metadata": {
        "id": "lLZMqPjisJAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "network_name='my_first_CNN_ViT'\n",
        "\n",
        "#Directories to train and validation datasets\n",
        "train_dataset_path='gs://.../*.record'\n",
        "validation_dataset_path='gs://.../*.record'\n",
        "\n",
        "#Directory for saving trained weights\n",
        "google_drive_base_dir='/content/gdrive/MyDrive/'\n",
        "weights_export_dir=google_drive_base_dir+network_name+'.h5'\n",
        "\n",
        "#Directory for checkpoints\n",
        "checkpoint_dir = 'gs://.../'+network_name+'/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
      ],
      "metadata": {
        "id": "fLAoePkhsSsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set seeds."
      ],
      "metadata": {
        "id": "s362hPVts_xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random as rnd\n",
        "\n",
        "rnd.seed(2)\n",
        "np.random.seed(3)\n",
        "tf.random.set_seed(1)"
      ],
      "metadata": {
        "id": "AsD0KQqjtDWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the TPU."
      ],
      "metadata": {
        "id": "A3DiGC1PtMSf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDK2IOWjO07I"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
        "            \n",
        "  print('Connection to TPU server successfull!')\n",
        "            \n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyUXcW9COvcu"
      },
      "outputs": [],
      "source": [
        "#A convinent way to provide access to Google Cloud Platform is to create a service account https://cloud.google.com/iam/docs/creating-managing-service-account-keys#iam-service-account-keys-create-console linked to the project\n",
        "#The procedure will download a .json file \n",
        "#Replace the fields below with the information contained in the file\n",
        "\n",
        "#If using TPU, it is also necessary to enable the TPU service account (service-[project_number]@cloud-tpu.iam.gserviceaccount.com) as an IAM user for the project\n",
        "\n",
        "import json\n",
        "\n",
        "data_all={\n",
        "  \"type\": \"service_account\",\n",
        "  \"project_id\": ,\n",
        "  \"private_key_id\": ,\n",
        "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n...==\\n-----END PRIVATE KEY-----\\n\",\n",
        "  \"client_email\": \"\",\n",
        "  \"client_id\": \"\",\n",
        "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
        "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
        "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
        "  \"client_x509_cert_url\": \"\"\n",
        "}\n",
        "\n",
        "parsed = json.dumps(data_all)\n",
        "\n",
        "with open('/content/.config/application_default_credentials.json', 'w') as f:\n",
        "  f.write(parsed)\n",
        "!gcloud auth activate-service-account --key-file '/content/.config/application_default_credentials.json'\n",
        "\n",
        "#Alternatively\n",
        "\n",
        "#!gcloud auth login\n",
        "#!gcloud config set project 'myproject' #set the project id here\n",
        "\n",
        "#from google.colab import auth\n",
        "#auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S0FZwtRKcnj"
      },
      "source": [
        "# Dataset processing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmBM4_UlKcJ7"
      },
      "outputs": [],
      "source": [
        "import tensorflow_addons as tfa\n",
        "\n",
        "#Load TFRecords files\n",
        "def load_tf_records(filepath):\n",
        "    ignore_order = tf.data.Options()\n",
        "    ignore_order.experimental_deterministic = True\n",
        "\n",
        "    filenames = tf.io.gfile.glob(filepath)\n",
        "    dataset = tf.data.TFRecordDataset(filenames,num_parallel_reads=tf.data.experimental.AUTOTUNE)\n",
        "    dataset = dataset.with_options(ignore_order)\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "\n",
        "#Define TFRecord structure\n",
        "\n",
        "def tf_records_file_features_description():\n",
        "    image_feature_description = {\n",
        "        'image/actual_channels': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
        "        \n",
        "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
        "        'image/format': tf.io.FixedLenFeature([], tf.string),\n",
        "\n",
        "        'image/object/kpts/X_A':tf.io.FixedLenFeature([], tf.float32),\n",
        "        'image/object/kpts/Y_A':tf.io.FixedLenFeature([], tf.float32),\n",
        "        'image/object/kpts/X_B':tf.io.FixedLenFeature([], tf.float32),\n",
        "        'image/object/kpts/Y_B':tf.io.FixedLenFeature([], tf.float32),\n",
        "        'image/object/kpts/X_C':tf.io.FixedLenFeature([], tf.float32),\n",
        "        'image/object/kpts/Y_C':tf.io.FixedLenFeature([], tf.float32),\n",
        "        'image/object/kpts/X_D':tf.io.FixedLenFeature([], tf.float32),\n",
        "        'image/object/kpts/Y_D':tf.io.FixedLenFeature([], tf.float32),\n",
        "        'image/object/kpts/X_E':tf.io.FixedLenFeature([], tf.float32),\n",
        "        'image/object/kpts/Y_E':tf.io.FixedLenFeature([], tf.float32),\n",
        "        'image/object/kpts/X_F':tf.io.FixedLenFeature([], tf.float32),\n",
        "        'image/object/kpts/Y_F':tf.io.FixedLenFeature([], tf.float32),\n",
        "        'image/object/kpts/X_G':tf.io.FixedLenFeature([], tf.float32),\n",
        "        'image/object/kpts/Y_G':tf.io.FixedLenFeature([], tf.float32),\n",
        "        'image/object/kpts/X_H':tf.io.FixedLenFeature([], tf.float32),\n",
        "        'image/object/kpts/Y_H':tf.io.FixedLenFeature([], tf.float32),\n",
        "        'image/object/kpts/X_I':tf.io.FixedLenFeature([], tf.float32),\n",
        "        'image/object/kpts/Y_I':tf.io.FixedLenFeature([], tf.float32),\n",
        "        'image/object/kpts/X_L':tf.io.FixedLenFeature([], tf.float32),\n",
        "        'image/object/kpts/Y_L':tf.io.FixedLenFeature([], tf.float32),\n",
        "        'image/object/kpts/X_M':tf.io.FixedLenFeature([], tf.float32),\n",
        "        'image/object/kpts/Y_M':tf.io.FixedLenFeature([], tf.float32),\n",
        "\n",
        "    }\n",
        "    return image_feature_description\n",
        "\n",
        "#Decode JPEG and resize (we will cache the output)\n",
        "def decode_dataset(example_proto,target_image_height,target_image_width):\n",
        "    features=tf.io.parse_single_example(example_proto, tf_records_file_features_description())\n",
        "    \n",
        "    raw_image = tf.io.decode_jpeg(features['image/encoded'],channels=0) #0: Use the number of channels in the JPEG-encoded image.\n",
        "    image=tf.image.resize(raw_image,\n",
        "                          [target_image_height,target_image_width],\n",
        "                          method=tf.image.ResizeMethod.BILINEAR,\n",
        "                          antialias=False\n",
        "    )\n",
        "    \n",
        "    return image, features\n",
        "\n",
        "def apply_augmentations(raw_image, features,target_image_height,target_image_width):\n",
        "\n",
        "    #Recover image features\n",
        "    image_height=tf.cast(features['image/height'],dtype=tf.float32)\n",
        "    image_width=tf.cast(features['image/width'],dtype=tf.float32)\n",
        "\n",
        "    #Principal point\n",
        "    cx = image_width/2.0\n",
        "    cy = image_height/2.0\n",
        "\n",
        "    X_A=features['image/object/kpts/X_A']-cx\n",
        "    Y_A=features['image/object/kpts/Y_A']-cy\n",
        "    X_B=features['image/object/kpts/X_B']-cx\n",
        "    Y_B=features['image/object/kpts/Y_B']-cy\n",
        "    X_C=features['image/object/kpts/X_C']-cx\n",
        "    Y_C=features['image/object/kpts/Y_C']-cy\n",
        "    X_D=features['image/object/kpts/X_D']-cx\n",
        "    Y_D=features['image/object/kpts/Y_D']-cy\n",
        "    X_E=features['image/object/kpts/X_E']-cx\n",
        "    Y_E=features['image/object/kpts/Y_E']-cy\n",
        "    X_F=features['image/object/kpts/X_F']-cx\n",
        "    Y_F=features['image/object/kpts/Y_F']-cy\n",
        "    X_G=features['image/object/kpts/X_G']-cx\n",
        "    Y_G=features['image/object/kpts/Y_G']-cy\n",
        "    X_H=features['image/object/kpts/X_H']-cx\n",
        "    Y_H=features['image/object/kpts/Y_H']-cy\n",
        "    X_I=features['image/object/kpts/X_I']-cx\n",
        "    Y_I=features['image/object/kpts/Y_I']-cy\n",
        "    X_L=features['image/object/kpts/X_L']-cx\n",
        "    Y_L=features['image/object/kpts/Y_L']-cy\n",
        "    X_M=features['image/object/kpts/X_M']-cx\n",
        "    Y_M=features['image/object/kpts/Y_M']-cy\n",
        "\n",
        "    rotation_angle= tf.random.uniform(\n",
        "        shape=[], minval=tf.constant(-np.pi), maxval=tf.constant(np.pi),seed=5000\n",
        "    )\n",
        "    \n",
        "    #Rotation matrix\n",
        "    cos = tf.cos(rotation_angle)\n",
        "    sin = tf.sin(rotation_angle)\n",
        "    R=tf.reshape([cos, sin, -sin,cos],[2,2])\n",
        "\n",
        "    [X_A,Y_A] = rotate_and_normalize_landmarks(R,X_A,Y_A,cx,cy,image_height,image_width)\n",
        "    [X_B,Y_B] = rotate_and_normalize_landmarks(R,X_B,Y_B,cx,cy,image_height,image_width)\n",
        "    [X_C,Y_C] = rotate_and_normalize_landmarks(R,X_C,Y_C,cx,cy,image_height,image_width)\n",
        "    [X_D,Y_D] = rotate_and_normalize_landmarks(R,X_D,Y_D,cx,cy,image_height,image_width)\n",
        "    [X_E,Y_E] = rotate_and_normalize_landmarks(R,X_E,Y_E,cx,cy,image_height,image_width)\n",
        "    [X_F,Y_F] = rotate_and_normalize_landmarks(R,X_F,Y_F,cx,cy,image_height,image_width)\n",
        "    [X_G,Y_G] = rotate_and_normalize_landmarks(R,X_G,Y_G,cx,cy,image_height,image_width)\n",
        "    [X_H,Y_H] = rotate_and_normalize_landmarks(R,X_H,Y_H,cx,cy,image_height,image_width)\n",
        "    [X_I,Y_I] = rotate_and_normalize_landmarks(R,X_I,Y_I,cx,cy,image_height,image_width)\n",
        "    [X_L,Y_L] = rotate_and_normalize_landmarks(R,X_L,Y_L,cx,cy,image_height,image_width)\n",
        "    [X_M,Y_M] = rotate_and_normalize_landmarks(R,X_M,Y_M,cx,cy,image_height,image_width)\n",
        "\n",
        "    #Rotate image\n",
        "    image=tfa.image.rotate(raw_image, rotation_angle)\n",
        "\n",
        "    #To RGB\n",
        "    image=tf.image.grayscale_to_rgb(image)\n",
        "\n",
        "    #Apply pixel level augmentations: edit the function pixel_level_augment\n",
        "    image = pixel_level_augment(image,target_image_height,target_image_width)\n",
        "    image = tf.clip_by_value(image,0,255)\n",
        "    \n",
        "    #Rescale\n",
        "    image=(image - 127.00) / 128.00\n",
        "\n",
        "    image = tf.reshape(image,[target_image_height,target_image_width,3])\n",
        "\n",
        "    output_kpts = [X_A, Y_A, X_B, Y_B, X_C, Y_C, X_D, Y_D, X_E,Y_E,X_F,Y_F, X_G, Y_G, X_H, Y_H, X_I, Y_I, X_L, Y_L, X_M, Y_M]\n",
        "    \n",
        "    return image,  { 'kpts_regressor': output_kpts}\n",
        "\n",
        "def rotate_and_normalize_landmarks(R,xp,yp,cx,cy,image_height,image_width):\n",
        "    \n",
        "    q=tf.tensordot(R,tf.stack([xp,yp]),axes=1)\n",
        "    xp=q[0]+cx\n",
        "    yp=q[1]+cy\n",
        "\n",
        "    xpn=xp/image_width\n",
        "    ypn=yp/image_height\n",
        "    \n",
        "    return xpn, ypn\n",
        "\n",
        "\n",
        "def pixel_level_augment(image,target_image_height,target_image_width): \n",
        "\n",
        "    op1 = tf.random.uniform([ ],maxval=4,dtype=tf.int32, seed=32)\n",
        "    image = tf.case([(tf.equal(op1,0),lambda: equalize(image)),\n",
        "                (tf.equal(op1,1),lambda: invert(image)),\n",
        "                (tf.equal(op1,2),lambda: posterize(image))],\n",
        "                default=lambda: solarize(image))\n",
        "\n",
        "    prob_brightness = tf.random.uniform([],minval=0,maxval=1,seed=49)\n",
        "    image = tf.cond(tf.less(prob_brightness,0.5), lambda: brightness(image, max_delta=0.5), lambda: image)\n",
        "\n",
        "    prob_contrast = tf.random.uniform([],minval=0,maxval=1,seed=76)\n",
        "    image = tf.cond(tf.less(prob_contrast,0.5), lambda: contrast(image,0.1,1.5), lambda: image)\n",
        "    \n",
        "    prob_blur = tf.random.uniform([],minval=0,maxval=1,seed=37)\n",
        "    image = tf.cond(tf.less(prob_blur,0.5), lambda: blurring(image, sigma=1), lambda: image)\n",
        "\n",
        "    prob_noise = tf.random.uniform([],minval=0,maxval=1,seed=42)\n",
        "    image = tf.cond(tf.less(prob_noise,0.5), lambda: add_gauss_noise(image,target_image_height,target_image_width), lambda: image)\n",
        "    \n",
        "    return image\n",
        "\n",
        "def brightness(image, max_delta):\n",
        "  return tf.image.random_brightness(image, max_delta=max_delta,seed=1)\n",
        "\n",
        "def contrast(image, min,max):\n",
        "  return tf.image.random_contrast(image,min,max,seed=2)\n",
        "\n",
        "def blurring(image,sigma):\n",
        "  return tfa.image.gaussian_filter2d(image, sigma)\n",
        "\n",
        "def noise(image, target_image_height,target_image_width):\n",
        "  return add_gauss_noise(image,target_image_height,target_image_width)\n",
        "\n",
        "def invert(image):\n",
        "  return tf.math.abs(255-image)\n",
        "\n",
        "def add_gauss_noise(image, target_image_height,target_image_width):\n",
        "      \n",
        "      mean = 0\n",
        "      var = tf.random.uniform([],minval=0, maxval=50,seed=52)\n",
        "      std = var**0.5\n",
        "      \n",
        "      gauss = tf.random.normal([target_image_height,target_image_width,3], mean,std,seed=65)\n",
        "\n",
        "      noisy = image + gauss\n",
        "      return noisy\n",
        "\n",
        "def equalize(image):\n",
        "  \"\"\"source: https://github.com/tensorflow/models/blob/master/official/vision/ops/augment.py\n",
        "  Implements Equalize function from PIL using TF ops.\"\"\"\n",
        "  def scale_channel(im, c):\n",
        "    \"\"\"Scale the data in the channel to implement equalize.\"\"\"\n",
        "    im = tf.cast(im[:, :, c], tf.int32)\n",
        "    # Compute the histogram of the image channel.\n",
        "    histo = tf.histogram_fixed_width(im, [0, 255], nbins=256)\n",
        "\n",
        "    # For the purposes of computing the step, filter out the nonzeros.\n",
        "    nonzero = tf.where(tf.not_equal(histo, 0))\n",
        "    nonzero_histo = tf.reshape(tf.gather(histo, nonzero), [-1])\n",
        "    step = (tf.reduce_sum(nonzero_histo) - nonzero_histo[-1]) // 255\n",
        "\n",
        "    def build_lut(histo, step):\n",
        "      # Compute the cumulative sum, shifting by step // 2\n",
        "      # and then normalization by step.\n",
        "      lut = (tf.cumsum(histo) + (step // 2)) // step\n",
        "      # Shift lut, prepending with 0.\n",
        "      lut = tf.concat([[0], lut[:-1]], 0)\n",
        "      # Clip the counts to be in range.  This is done\n",
        "      # in the C code for image.point.\n",
        "      return tf.clip_by_value(lut, 0, 255)\n",
        "\n",
        "    # If step is zero, return the original image.  Otherwise, build\n",
        "    # lut from the full histogram and step and then index from it.\n",
        "    result = tf.cond(tf.equal(step, 0),\n",
        "                     lambda: im,\n",
        "                     lambda: tf.gather(build_lut(histo, step), im))\n",
        "\n",
        "    return tf.cast(result, tf.float32)\n",
        "\n",
        "  # Assumes RGB for now.  Scales each channel independently\n",
        "  # and then stacks the result.\n",
        "  s1 = scale_channel(image, 0)\n",
        "  s2 = scale_channel(image, 1)\n",
        "  s3 = scale_channel(image, 2)\n",
        "  image = tf.stack([s1, s2, s3], 2)\n",
        "  return image\n",
        "\n",
        "def solarize(image, threshold=128.):\n",
        "  \"\"\"source: https://github.com/tensorflow/models/blob/master/official/vision/ops/augment.py\"\"\"\n",
        "  # For each pixel in the image, select the pixel\n",
        "  # if the value is less than the threshold.\n",
        "  # Otherwise, subtract 255 from the pixel.\n",
        "  return tf.where(image < threshold, image, 255. - image)\n",
        "\n",
        "\n",
        "def posterize(image,bits=2):\n",
        "  \"\"\"source: https://github.com/tensorflow/models/blob/master/official/vision/ops/augment.py\n",
        "  Equivalent of PIL Posterize.\"\"\"\n",
        "  image=tf.cast(image,tf.uint8)\n",
        "  shift = 8 - bits\n",
        "  return tf.cast(tf.bitwise.left_shift(tf.bitwise.right_shift(image, shift), shift),tf.float32)\n",
        "\n",
        "\n",
        "def map_validation_dataset(image, features, target_image_height,target_image_width):\n",
        "    \n",
        "    image_height=tf.cast(features['image/height'],dtype=tf.float32)\n",
        "    image_width=tf.cast(features['image/width'],dtype=tf.float32)\n",
        "\n",
        "    X_A=features['image/object/kpts/X_A']/image_width\n",
        "    Y_A=features['image/object/kpts/Y_A']/image_height\n",
        "    X_B=features['image/object/kpts/X_B']/image_width\n",
        "    Y_B=features['image/object/kpts/Y_B']/image_height\n",
        "    X_C=features['image/object/kpts/X_C']/image_width\n",
        "    Y_C=features['image/object/kpts/Y_C']/image_height\n",
        "    X_D=features['image/object/kpts/X_D']/image_width\n",
        "    Y_D=features['image/object/kpts/Y_D']/image_height\n",
        "    X_E=features['image/object/kpts/X_E']/image_width\n",
        "    Y_E=features['image/object/kpts/Y_E']/image_height\n",
        "    X_F=features['image/object/kpts/X_F']/image_width\n",
        "    Y_F=features['image/object/kpts/Y_F']/image_height\n",
        "    X_G=features['image/object/kpts/X_G']/image_width\n",
        "    Y_G=features['image/object/kpts/Y_G']/image_height\n",
        "    X_H=features['image/object/kpts/X_H']/image_width\n",
        "    Y_H=features['image/object/kpts/Y_H']/image_height\n",
        "    X_I=features['image/object/kpts/X_I']/image_width\n",
        "    Y_I=features['image/object/kpts/Y_I']/image_height\n",
        "    X_L=features['image/object/kpts/X_L']/image_width\n",
        "    Y_L=features['image/object/kpts/Y_L']/image_height\n",
        "    X_M=features['image/object/kpts/X_M']/image_width\n",
        "    Y_M=features['image/object/kpts/Y_M']/image_height\n",
        "\n",
        "    image=tf.image.grayscale_to_rgb(image)\n",
        "    image = tf.cast(image, tf.float32)\n",
        "\n",
        "    image=(image - 127.00) / 128.00\n",
        "    image = tf.reshape(image, [target_image_height, target_image_width, 3])\n",
        "    \n",
        "    output_kpts = [X_A, Y_A, X_B, Y_B, X_C, Y_C, X_D, Y_D, X_E,Y_E,X_F,Y_F, X_G, Y_G, X_H, Y_H, X_I, Y_I, X_L, Y_L, X_M, Y_M]\n",
        "    \n",
        "    return image, { 'kpts_regressor': output_kpts}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Optional) Visualize the dataset\n",
        "Use the following cells to visualize the dataset and check that everything is fine."
      ],
      "metadata": {
        "id": "BaHkA7Mazqt6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-rLZTnULcY_"
      },
      "outputs": [],
      "source": [
        "input_shape=[320, 512, 3]\n",
        "\n",
        "height = input_shape[0]\n",
        "width = input_shape[1]\n",
        "\n",
        "AUTO=tf.data.AUTOTUNE\n",
        "\n",
        "train_dataset=load_tf_records(train_dataset_path).map(lambda x : decode_dataset(x, height,width), num_parallel_calls=AUTO).map(lambda x,y: apply_augmentations(x,y,height,width),num_parallel_calls=AUTO)\n",
        "\n",
        "validation_dataset=load_tf_records(validation_dataset_path).map(lambda x: decode_dataset(x, height,width), num_parallel_calls=AUTO).map(lambda x, y: map_validation_dataset(x,y, height,width), num_parallel_calls=AUTO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADLrQd3sLfec"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "even=np.arange(0,22,2)\n",
        "odd=np.arange(1,22,2)\n",
        "\n",
        "for image, label in train_dataset.take(10):\n",
        "  \n",
        "  plt.imshow((image*128.0+127.0)/255.0)\n",
        "  plt.plot(label['kpts_regressor'].numpy()[even]*width,label['kpts_regressor'].numpy()[odd]*height,'.')\n",
        "\n",
        "  plt.show()\n",
        "  print(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-Aft6eui_ml"
      },
      "source": [
        "# Model building"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the encoder (EfficientNet backbone + ViT)."
      ],
      "metadata": {
        "id": "GBEGXbFxHJcs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_Ww-bFzAaBv"
      },
      "outputs": [],
      "source": [
        "from models_and_layers.efficientnet_lite import EfficientNetLiteB4\n",
        "from models_and_layers.vit_layers import AddPositionEmbs, TransformerBlock\n",
        "\n",
        "#Code adapted from https://github.com/faustomorales/vit-keras\n",
        "#Licensed under Apache 2.0 license\n",
        "#Removed classToken\n",
        "\n",
        "def build_encoder(\n",
        "    input_shape=(320, 512, 3),\n",
        "    patch_size=4,\n",
        "    num_layers=6,\n",
        "    hidden_size=256,\n",
        "    num_heads=8,\n",
        "    mlp_dim=2048,\n",
        "    dropout=0.1\n",
        "):\n",
        "    \"\"\"Build transformer encoder.\n",
        "\n",
        "    Args:\n",
        "        input_shape: The size of input images.\n",
        "        patch_size: The size of each patch (must fit evenly in image_size)\n",
        "        num_layers: The number of transformer layers to use.\n",
        "        hidden_size: The number of filters to use\n",
        "        num_heads: The number of transformer heads\n",
        "        mlp_dim: The number of dimensions for the MLP output in the transformers.\n",
        "        dropout_rate: fraction of the units to drop for dense layers.\n",
        "    \"\"\"\n",
        "    \n",
        "    inputlayer=tf.keras.layers.Input(shape=(input_shape[0], input_shape[1], 3))\n",
        "\n",
        "    model = EfficientNetLiteB4(weights='imagenet', input_shape=(input_shape[0], input_shape[1], 3),include_top=False)(inputlayer)\n",
        "    #model=tf.keras.models.Model(inputs=model.input,outputs=model.layers[-1].output)(inputlayer)\n",
        "    #x = tf.keras.layers.Conv2D(64,1)(model)\n",
        "    #x = tf.keras.layers.Input(shape=(image_size[0], image_size[1], 3))\n",
        "    y = tf.keras.layers.Conv2D(\n",
        "        filters=hidden_size,\n",
        "        kernel_size=patch_size,\n",
        "        strides=patch_size,\n",
        "        padding=\"valid\",\n",
        "        name=\"embedding\",\n",
        "        kernel_initializer=tf.keras.initializers.GlorotUniform(seed=1116),\n",
        "    )(model)\n",
        "    y = tf.keras.layers.Reshape((y.shape[1] * y.shape[2], hidden_size))(y)\n",
        "\n",
        "    y = AddPositionEmbs(name=\"Transformer/posembed_input\")(y)\n",
        "    for n in range(num_layers):\n",
        "        y, _ = TransformerBlock(\n",
        "            num_heads=num_heads,\n",
        "            mlp_dim=mlp_dim,\n",
        "            dropout=dropout,\n",
        "            name=f\"Transformer/encoderblock_{n}\",\n",
        "        )(y)\n",
        "    y = tf.keras.layers.LayerNormalization(\n",
        "        epsilon=1e-6, name=\"Transformer/encoder_norm\"\n",
        "    )(y)\n",
        "\n",
        "    y=tf.keras.layers.GlobalAveragePooling1D()(y)\n",
        "    #y=tf.keras.layers.Flatten()(y)\n",
        "    \n",
        "    return tf.keras.models.Model(inputs=inputlayer, outputs=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize regression head."
      ],
      "metadata": {
        "id": "mMrEog10IJcb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuB8QpAonj7S"
      },
      "outputs": [],
      "source": [
        "class kpts_regressor(tf.keras.Model):\n",
        "  def __init__(self,hidden_dim,num_keypoints):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_keypoints = num_keypoints\n",
        "        self.basic_layers = tf.keras.Sequential(\n",
        "            [tf.keras.layers.Dropout(0.1,seed=43),\n",
        "             tf.keras.layers.Dense(self.hidden_dim,activation='gelu',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=9001)),\n",
        "          tf.keras.layers.Dropout(0.1,seed=819),\n",
        "          tf.keras.layers.Dense(self.hidden_dim/2,activation='gelu',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=901)),\n",
        "          tf.keras.layers.Dense(22,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=976),name='kpts'),\n",
        "         ]\n",
        "        )\n",
        "  def call(self, x):\n",
        "    x = self.basic_layers(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the model."
      ],
      "metadata": {
        "id": "0TB-8MY0ICz8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSZDkBXu0Ufb"
      },
      "outputs": [],
      "source": [
        "#Vit tiny:\n",
        "hidden_dim=192\n",
        "num_keypoints = 11\n",
        "input_shape=[320, 512, 3]\n",
        "inputlayer=tf.keras.layers.Input(shape=(input_shape[0], input_shape[1], 3))\n",
        "\n",
        "with tpu_strategy.scope(): \n",
        "  encoder=test=build_encoder(input_shape=(320, 512, 3),\n",
        "    patch_size=1,\n",
        "    num_layers=1,\n",
        "    hidden_size=hidden_dim,\n",
        "    num_heads=3,\n",
        "    mlp_dim=hidden_dim*3,\n",
        "    dropout=0.1\n",
        "  )(inputlayer)\n",
        "  encoder=tf.keras.models.Model([inputlayer], [encoder])\n",
        "  regressor_kpts = kpts_regressor(hidden_dim,num_keypoints)(encoder.output)\n",
        "  network=tf.keras.models.Model([encoder.input], [regressor_kpts])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize NN details."
      ],
      "metadata": {
        "id": "jZNE_JzwIF_b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGNHcSHDRiQt"
      },
      "outputs": [],
      "source": [
        "network.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpXaXzZ7SBTg"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(network,show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PSQdc_iEHPg"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset preprocessing."
      ],
      "metadata": {
        "id": "AtQAHzilImVk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmVyPK8cQFyR"
      },
      "outputs": [],
      "source": [
        "batch_size=64\n",
        "epochs = 80\n",
        "\n",
        "height = input_shape[0]\n",
        "width = input_shape[1]\n",
        "AUTO=tf.data.AUTOTUNE\n",
        "\n",
        "# Train dataset preparation\n",
        "\n",
        "all_train_record=load_tf_records(train_dataset_path).map(lambda x : decode_dataset(x, height,width), num_parallel_calls=AUTO).cache().shuffle(15000,seed=29).map(lambda x,y: apply_augmentations(x,y,height,width),num_parallel_calls=AUTO)\n",
        "\n",
        "train_dataset = all_train_record.batch(batch_size,drop_remainder=True).repeat()\n",
        "\n",
        "test_dataset=load_tf_records(validation_dataset_path).map(lambda x: decode_dataset(x, height,width), num_parallel_calls=AUTO).map(lambda x, y: map_validation_dataset(x,y, height,width), num_parallel_calls=AUTO).batch(batch_size,drop_remainder=True).cache().repeat().prefetch(AUTO)\n",
        "\n",
        "\n",
        "steps_per_epoch=np.round(47966//batch_size)\n",
        "validation_steps=np.round(2791//batch_size) #sunlamp images"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the model."
      ],
      "metadata": {
        "id": "75Yz_uiJJDNV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfhTF3r3jwvj"
      },
      "outputs": [],
      "source": [
        "total_steps = steps_per_epoch*epochs\n",
        "base_lr=1e-4\n",
        "with tpu_strategy.scope(): \n",
        "\n",
        "  optimizer=tfa.optimizers.AdamW(weight_decay=1e-8,\n",
        "      learning_rate=tf.keras.optimizers.schedules.CosineDecay(base_lr, total_steps)\n",
        "  )\n",
        "\n",
        "  losses={\"kpts_regressor\": 'MAE',\n",
        "                }\n",
        "  network.compile(optimizer=optimizer,\n",
        "                  loss=losses,\n",
        "  )\n",
        "\n",
        "#Callbacks\n",
        "\n",
        "#Learning rate callback\n",
        "logger=tf.get_logger()\n",
        "class LearningRateLoggingCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch,logs={}):\n",
        "        lr = self.model.optimizer._decayed_lr(tf.float32)\n",
        "        logger.info(\"lr value = %s\" % lr)\n",
        "\n",
        "#Backup and restore callback\n",
        "backup_and_restore_callback=tf.keras.callbacks.BackupAndRestore(\n",
        "    backup_dir=checkpoint_dir\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "network.fit(train_dataset,\n",
        "        validation_data=test_dataset,\n",
        "        epochs=epochs ,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_steps=validation_steps,\n",
        "        callbacks=[LearningRateLoggingCallback(),backup_and_restore_callback],\n",
        "        verbose=2\n",
        "       )\n",
        "\n",
        "network.save_weights(weights_export_dir)"
      ],
      "metadata": {
        "id": "wTd0bco2dhJE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CNN_ViT_train.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}