{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LRN_train_adversarial_domain_adaptation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["# Adversarial Training LRNs"],"metadata":{"id":"XMY5diAaEFxS"}},{"cell_type":"markdown","source":["This code illustrates our method for adversarial training of LRNs. The notebook is configured for running on a TPU hosted runtime on Google Colab."],"metadata":{"id":"ypxERA1k-XLg"}},{"cell_type":"markdown","source":["# Preliminaries"],"metadata":{"id":"_k0lYVKT9taw"}},{"cell_type":"markdown","source":["Install required packages."],"metadata":{"id":"EvhATr_yIds0"}},{"cell_type":"code","source":["!pip install git+https://github.com/Microsatellites-and-Space-Microsystems/pose_estimation_domain_gap --quiet"],"metadata":{"id":"L_xU5qFQIdVs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Provide access to Google Drive."],"metadata":{"id":"SMmS-51xAS_z"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"kVCqpKVRAQCG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Set network name and dataset directories."],"metadata":{"id":"0Rgb708zAcyE"}},{"cell_type":"code","source":["import os\n","\n","network_name='my_first_LRN'\n","\n","#Directories to train and validation datasets\n","train_dataset_path='gs://.../*.record'\n","validation_dataset_path='gs://.../*.record'\n","\n","#Directory for saving trained weights\n","google_drive_base_dir='/content/gdrive/MyDrive/'\n","weights_export_dir=google_drive_base_dir+network_name+'.h5'\n","\n","#Directory for checkpoints\n","checkpoint_dir = 'gs://.../'+network_name+'/training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"],"metadata":{"id":"0LUj2ugpAKBt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Set seeds."],"metadata":{"id":"yp2RjKuc-jtV"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import random as rnd\n","\n","rnd.seed(242)\n","np.random.seed(312)\n","tf.random.set_seed(112)"],"metadata":{"id":"VIestnrX-lqM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Initialize the TPU."],"metadata":{"id":"0Foz4N5v-RHn"}},{"cell_type":"code","source":["try:\n","  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n","            \n","  print('Connection to TPU server successfull!')\n","            \n","except ValueError:\n","  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n","\n","tf.config.experimental_connect_to_cluster(tpu)\n","tf.tpu.experimental.initialize_tpu_system(tpu)\n","tpu_strategy = tf.distribute.TPUStrategy(tpu)"],"metadata":{"id":"PDK2IOWjO07I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To train the NNs with Cloud TPUs, the dataset must be stored in a Cloud Bucket. Then it is necessary to give the TPU access to the Bucket."],"metadata":{"id":"gquMGYpb98s3"}},{"cell_type":"code","source":["#A convinent way to provide access to Google Cloud Platform is to create a service account https://cloud.google.com/iam/docs/creating-managing-service-account-keys#iam-service-account-keys-create-console linked to the project\n","#The procedure will download a .json file \n","#Replace the fields below with the information contained in the file\n","\n","#If using TPU, it is also necessary to enable the TPU service account (service-[project_number]@cloud-tpu.iam.gserviceaccount.com) as an IAM user for the project\n","\n","import json\n","\n","data_all={\n","  \"type\": \"service_account\",\n","  \"project_id\": ,\n","  \"private_key_id\": ,\n","  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n...==\\n-----END PRIVATE KEY-----\\n\",\n","  \"client_email\": \"\",\n","  \"client_id\": \"\",\n","  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n","  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n","  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n","  \"client_x509_cert_url\": \"\"\n","}\n","\n","parsed = json.dumps(data_all)\n","\n","with open('/content/.config/application_default_credentials.json', 'w') as f:\n","  f.write(parsed)\n","!gcloud auth activate-service-account --key-file '/content/.config/application_default_credentials.json'\n","\n","#Alternatively\n","\n","#!gcloud auth login\n","#!gcloud config set project 'myproject' #set the project id here\n","\n","#from google.colab import auth\n","#auth.authenticate_user()"],"metadata":{"id":"dyUXcW9COvcu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Initialize LRN Swin based"],"metadata":{"id":"wh9zgDaj2SrA"}},{"cell_type":"markdown","source":["Initialize the NN encoder. To try different backbones modify the imported model in the first line of the following cell."],"metadata":{"id":"8tylqWbn-zTh"}},{"cell_type":"code","source":["from models_and_layers.tfswin import SwinTransformerTiny224 as transformerEncoder\n","with tpu_strategy.scope(): \n","\n","  def get_encoder(input_shape):\n","    \n","    input = tf.keras.layers.Input(shape=(input_shape, input_shape, 3))\n","    model=transformerEncoder(include_top=False)(input)\n","    x = tf.keras.layers.GlobalAveragePooling2D(name='avg_pool')(model)\n","    model = tf.keras.models.Model(inputs=input, outputs=x)\n","    \n","    return model"],"metadata":{"id":"JOzkVcuJ2YEc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Initialize discriminator and regressor heads."],"metadata":{"id":"sDo7bcA2-20h"}},{"cell_type":"code","source":["class Discriminator(tf.keras.Model):\n","  def __init__(self,hidden_dim):\n","        super().__init__()\n","        \n","        self.hidden_dim = hidden_dim\n","        self.basic_layers = tf.keras.Sequential(\n","          [tf.keras.layers.Dense(self.hidden_dim*4,activation='gelu',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=1509)),\n","          tf.keras.layers.Dense(self.hidden_dim,activation='gelu',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=9)),\n","          tf.keras.layers.Dense(1,name='cls',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=3412)),\n","          ]\n","        )\n","  def call(self, x):\n","    x = self.basic_layers(x)\n","    return x\n","  \n","class Regressor(tf.keras.Model):\n","  def __init__(self,hidden_dim,num_keypoints):\n","        super().__init__()\n","        \n","        self.hidden_dim = hidden_dim\n","        self.num_keypoints = num_keypoints\n","        self.basic_layers = tf.keras.Sequential(\n","        [tf.keras.layers.Dense(hidden_dim*4,activation='gelu',name='kpts1',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=121)),\n","         tf.keras.layers.Dense(hidden_dim,activation='gelu',name='kpts2',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=432)),\n","         tf.keras.layers.Dense(num_keypoints*2,activation='linear',name='kpts',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=3454)),\n","         ]\n","        )\n","  def call(self, x):\n","    x = self.basic_layers(x)\n","    return x"],"metadata":{"id":"PuB8QpAonj7S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Build the model. "],"metadata":{"id":"MO6C3H4D-7bm"}},{"cell_type":"code","source":["input_shape = 224  #Assumed squared\n","hidden_dim=768     #Output size of the encoder\n","num_keypoints = 11 #Satellite's keypoints\n","\n","with tpu_strategy.scope(): \n","  encoder=get_encoder(input_shape)\n","  discriminator = Discriminator(hidden_dim)(encoder.output)\n","  regressor=Regressor(hidden_dim,num_keypoints)(encoder.output)\n","  network=tf.keras.models.Model([encoder.input], [discriminator,regressor])\n","\n"],"metadata":{"id":"pmCAmUZ5RAWt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualize the NN details."],"metadata":{"id":"sf63HLwIBd0u"}},{"cell_type":"code","source":["network.summary()"],"metadata":{"id":"OGNHcSHDRiQt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.keras.utils.plot_model(network,show_shapes=True)"],"metadata":{"id":"XpXaXzZ7SBTg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Initialize LRN EfficientNet based"],"metadata":{"id":"aNbvSg3DtXYl"}},{"cell_type":"code","source":["from models_and_layers.efficientnet import EfficientNetV1B5\n","\n","class get_encoder(tf.keras.Model):\n","  def __init__(self,hidden_dim,input_shape):\n","        super().__init__()\n","        \n","        self.hidden_dim = hidden_dim\n","        self.num_keypoints = num_keypoints\n","        self.basic_layers = tf.keras.Sequential([\n","    EfficientNetV1B5(num_classes=0,input_shape=(input_shape,input_shape,3),pretrained=\"imagenet\"),\n","    tf.keras.layers.Conv2D(self.hidden_dim,1,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=231)),\n","    tf.keras.layers.GlobalAveragePooling2D(name='avg_pool')])\n","         \n","  def call(self, x):\n","    x = self.basic_layers(x)\n","    return x"],"metadata":{"id":"JWoYhpqWtduw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Discriminator(tf.keras.Model):\n","  def __init__(self,hidden_dim):\n","        super().__init__()\n","        \n","        self.hidden_dim = hidden_dim\n","        self.basic_layers = tf.keras.Sequential(\n","          [tf.keras.layers.Dense(self.hidden_dim*4,activation='gelu',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=1509)),\n","          tf.keras.layers.Dense(self.hidden_dim,activation='gelu',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=9)),\n","          tf.keras.layers.Dense(1,name='cls',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=3412)),\n","          ]\n","        )\n","  def call(self, x):\n","    x = self.basic_layers(x)\n","    return x\n","  \n","class Regressor(tf.keras.Model):\n","  def __init__(self,hidden_dim,num_keypoints):\n","        super().__init__()\n","        \n","        self.hidden_dim = hidden_dim\n","        self.num_keypoints = num_keypoints\n","        self.basic_layers = tf.keras.Sequential(\n","        [tf.keras.layers.Dense(hidden_dim*4,activation='gelu',name='kpts1',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=121)),\n","         tf.keras.layers.Dense(hidden_dim,activation='gelu',name='kpts2',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=432)),\n","         tf.keras.layers.Dense(num_keypoints*2,activation='linear',name='kpts',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=3454)),\n","         ]\n","        )\n","  def call(self, x):\n","    x = self.basic_layers(x)\n","    return x"],"metadata":{"id":"vtnZuZ-ZtqnX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build the model:\n","\n","input_shape = 224  #Assumed squared\n","hidden_dim=768     #Output size of the encoder\n","num_keypoints = 11 #Satellite's keypoints\n","input=tf.keras.layers.Input(shape=(input_shape, input_shape, 3))\n","\n","with tpu_strategy.scope(): \n","  encoder=get_encoder(hidden_dim,input_shape)\n","  discriminator = Discriminator(hidden_dim)(encoder(input))\n","  regressor=Regressor(hidden_dim,num_keypoints)(encoder(input))\n","  network=tf.keras.models.Model([encoder.input], [discriminator,regressor])\n"],"metadata":{"id":"bF208WDGtuWv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualize NN details."],"metadata":{"id":"b_yGMv5wt7q3"}},{"cell_type":"code","source":["network.summary()"],"metadata":{"id":"Zynirc5pt99R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.keras.utils.plot_model(network,show_shapes=True)"],"metadata":{"id":"qv9yCYEDuApK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset processing\n","The following cells contain all the functions to preprocess the dataset."],"metadata":{"id":"g4KHG-NhDFIL"}},{"cell_type":"code","source":["import tensorflow_addons as tfa\n","\n","#Load TFRecords files\n","\n","def load_tf_records(filepath):\n","    ignore_order = tf.data.Options()\n","    ignore_order.experimental_deterministic = True\n","\n","    filenames = tf.io.gfile.glob(filepath)\n","    dataset = tf.data.TFRecordDataset(filenames,num_parallel_reads=tf.data.experimental.AUTOTUNE)\n","    dataset = dataset.with_options(ignore_order)\n","    \n","    return dataset\n","\n","#Define TFRecord structure\n","\n","def tf_records_file_features_description():\n","    image_feature_description = {\n","        'image/actual_channels': tf.io.FixedLenFeature([], tf.int64),\n","        'image/height': tf.io.FixedLenFeature([], tf.int64),\n","        'image/dataset_class': tf.io.FixedLenFeature([], tf.int64),\n","        'image/width': tf.io.FixedLenFeature([], tf.int64),\n","        'image/filename': tf.io.FixedLenFeature([], tf.string),\n","        \n","        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n","        'image/format': tf.io.FixedLenFeature([], tf.string),\n","\n","\n","        'image/object/bbox/xmin':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/bbox/xmax':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/bbox/ymin':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/bbox/ymax':tf.io.FixedLenFeature([], tf.float32),\n","\n","        'image/object/kpts/X_A':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/Y_A':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/X_B':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/Y_B':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/X_C':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/Y_C':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/X_D':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/Y_D':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/X_E':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/Y_E':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/X_F':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/Y_F':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/X_G':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/Y_G':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/X_H':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/Y_H':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/X_I':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/Y_I':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/X_L':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/Y_L':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/X_M':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/Y_M':tf.io.FixedLenFeature([], tf.float32),\n","    }\n","    return image_feature_description\n","\n","#Decode JPEG first (we will cache the output)\n","\n","def decode_dataset(example_proto,image_size):\n","    features=tf.io.parse_single_example(example_proto, tf_records_file_features_description())\n","    \n","    raw_image = tf.io.decode_jpeg(features['image/encoded'],channels=0) #0: Use the number of channels in the JPEG-encoded image.\n","    \n","    return raw_image, features\n","\n","#Apply augmentations during training\n","\n","def apply_augmentations(raw_image, features,target_image_size):\n","\n","    #Recover image features\n","    image_height=tf.cast(features['image/height'],dtype=tf.float32)\n","    image_width=tf.cast(features['image/width'],dtype=tf.float32)\n","    dataset_class=tf.cast(features['image/dataset_class'],dtype=tf.float32)\n","\n","    xmin=features['image/object/bbox/xmin']\n","    ymin=features['image/object/bbox/ymin']\n","    xmax=features['image/object/bbox/xmax']\n","    ymax=features['image/object/bbox/ymax']\n","\n","    #Principal point\n","    cx=image_width/2.0\n","    cy=image_height/2.0\n","\n","    #The image will be rotated wrt the center point \n","    X_A=features['image/object/kpts/X_A']-cx\n","    Y_A=features['image/object/kpts/Y_A']-cy\n","    X_B=features['image/object/kpts/X_B']-cx\n","    Y_B=features['image/object/kpts/Y_B']-cy\n","    X_C=features['image/object/kpts/X_C']-cx\n","    Y_C=features['image/object/kpts/Y_C']-cy\n","    X_D=features['image/object/kpts/X_D']-cx\n","    Y_D=features['image/object/kpts/Y_D']-cy\n","    X_E=features['image/object/kpts/X_E']-cx\n","    Y_E=features['image/object/kpts/Y_E']-cy\n","    X_F=features['image/object/kpts/X_F']-cx\n","    Y_F=features['image/object/kpts/Y_F']-cy\n","    X_G=features['image/object/kpts/X_G']-cx\n","    Y_G=features['image/object/kpts/Y_G']-cy\n","    X_H=features['image/object/kpts/X_H']-cx\n","    Y_H=features['image/object/kpts/Y_H']-cy\n","    X_I=features['image/object/kpts/X_I']-cx\n","    Y_I=features['image/object/kpts/Y_I']-cy\n","    X_L=features['image/object/kpts/X_L']-cx\n","    Y_L=features['image/object/kpts/Y_L']-cy\n","    X_M=features['image/object/kpts/X_M']-cx\n","    Y_M=features['image/object/kpts/Y_M']-cy\n","\n","    #Random rotation angle\n","    rotation_angle= tf.random.uniform(\n","        shape=[], minval=tf.constant(-np.pi), maxval=tf.constant(np.pi),seed=5000\n","    )\n","    \n","    #Rotation matrix\n","    cos = tf.cos(rotation_angle)\n","    sin = tf.sin(rotation_angle)\n","    R=tf.reshape([cos, sin, -sin,cos],[2,2])\n","    \n","    #Rotate the bounding box\n","    q=tf.matmul(R,tf.reshape([xmin-cx,xmin-cx,xmax-cx,xmax-cx,\n","                              ymin-cy,ymax-cy,ymin-cy,ymax-cy],[2,4]))\n","\n","    #tl = top left, bl = bottom left, tr = top right, br = bottom right\n","    xtl=q[0,0]\n","    ytl=q[1,0]\n","    xbl=q[0,1]\n","    ybl=q[1,1]\n","    xtr=q[0,2]\n","    ytr=q[1,2]\n","    xbr=q[0,3]\n","    ybr=q[1,3]\n","    \n","    #Recover rotated bbox coordinates in original image frame\n","    \n","    xmin_rotated=tf.reduce_min([xtl,xbl,xtr,xbr])+cx\n","    xmax_rotated=tf.reduce_max([xtl,xbl,xtr,xbr])+cx\n","    ymin_rotated=tf.reduce_min([ytl,ybl,ytr,ybr])+cy\n","    ymax_rotated=tf.reduce_max([ytl,ybl,ytr,ybr])+cy      \n","\n","    #Clip the values between 0 and the original image dimensions\n","    xmin_rotated=tf.maximum(xmin_rotated,0.0)\n","    xmax_rotated=tf.minimum(xmax_rotated,image_width)\n","    ymin_rotated=tf.maximum(ymin_rotated,0.0)\n","    ymax_rotated=tf.minimum(ymax_rotated,image_height)\n","\n","    #Reconstruct the rotated (and clipped) GT bbox\n","    #Width, height and center coordinates\n","    gt_w=xmax_rotated-xmin_rotated\n","    gt_h=ymax_rotated-ymin_rotated\n","    x_c=(xmax_rotated+xmin_rotated)/2.0\n","    y_c=(ymax_rotated+ymin_rotated)/2.0\n","\n","    #The cropping region is made square to avoid image distortion\n","    #In general we take the largest size\n","    bbox_side=tf.cond(tf.greater(gt_w,gt_h),lambda: gt_w, lambda: gt_h)\n","    \n","    #Consider the case where the bbox is greater than the image height\n","    bbox_side=tf.cond(tf.greater(bbox_side,image_height),lambda: image_height,lambda: bbox_side)\n","    y_c=tf.cond(tf.equal(bbox_side,image_height),lambda: cy, lambda: y_c)\n","    \n","    #Move the bbox vertices\n","    ymin_rotated=tf.cond(tf.equal(bbox_side,image_height),lambda: y_c-image_height/2,lambda: ymin_rotated)\n","    ymax_rotated=tf.cond(tf.equal(bbox_side,image_height),lambda: y_c+image_height/2,lambda: ymax_rotated)\n","    xmin_rotated=tf.cond(tf.equal(bbox_side,image_height),lambda: x_c-image_height/2,lambda: xmin_rotated)\n","    xmax_rotated=tf.cond(tf.equal(bbox_side,image_height),lambda: x_c+image_height/2,lambda: xmax_rotated)\n","\n","    #The bounding box is randomly re-scaled and translated\n","    [xmin_new, ymin_new, bbox_side]=scale_and_translate(xmin_rotated,xmax_rotated,ymin_rotated,ymax_rotated,bbox_side,x_c,y_c,target_image_size,image_height,image_width)\n","    \n","    #Crop to bounding box requires integers\n","    ymin_new=tf.floor(ymin_new)\n","    xmin_new=tf.floor(xmin_new)\n","    bbox_side=tf.floor(bbox_side)\n","    \n","    bbox_side=tf.cond(tf.greater(ymin_new+bbox_side,image_height),lambda: tf.floor(image_height-ymin_new), lambda: bbox_side)\n","    bbox_side=tf.cond(tf.greater(xmin_new+bbox_side,image_width),lambda: tf.floor(image_width-xmin_new), lambda: bbox_side)\n","    \n","    #Rotate image\n","    image=tfa.image.rotate(raw_image, rotation_angle)\n","\n","    #Crop image\n","    image=tf.image.crop_to_bounding_box(image,\n","                                        tf.cast(ymin_new,tf.int32),\n","                                        tf.cast(xmin_new,tf.int32),\n","                                        tf.cast(bbox_side,tf.int32),\n","                                        tf.cast(bbox_side,tf.int32),\n","                                        )\n","    \n","    #Resize image to fit CNN requirements\n","    image=tf.image.resize(image,\n","                          [target_image_size,target_image_size],\n","                          method=tf.image.ResizeMethod.BILINEAR,\n","                          antialias=False\n","    )\n","    \n","   \n","    #Rotation of all keypoints and coordinates normalization\n","    [X_A,Y_A] = rotate_and_normalize_landmarks(R,X_A,Y_A,cx,cy,xmin_new,ymin_new,bbox_side)\n","    [X_B,Y_B] = rotate_and_normalize_landmarks(R,X_B,Y_B,cx,cy,xmin_new,ymin_new,bbox_side)\n","    [X_C,Y_C] = rotate_and_normalize_landmarks(R,X_C,Y_C,cx,cy,xmin_new,ymin_new,bbox_side)\n","    [X_D,Y_D] = rotate_and_normalize_landmarks(R,X_D,Y_D,cx,cy,xmin_new,ymin_new,bbox_side)\n","    [X_E,Y_E] = rotate_and_normalize_landmarks(R,X_E,Y_E,cx,cy,xmin_new,ymin_new,bbox_side)\n","    [X_F,Y_F] = rotate_and_normalize_landmarks(R,X_F,Y_F,cx,cy,xmin_new,ymin_new,bbox_side)\n","    [X_G,Y_G] = rotate_and_normalize_landmarks(R,X_G,Y_G,cx,cy,xmin_new,ymin_new,bbox_side)\n","    [X_H,Y_H] = rotate_and_normalize_landmarks(R,X_H,Y_H,cx,cy,xmin_new,ymin_new,bbox_side)\n","    [X_I,Y_I] = rotate_and_normalize_landmarks(R,X_I,Y_I,cx,cy,xmin_new,ymin_new,bbox_side)\n","    [X_L,Y_L] = rotate_and_normalize_landmarks(R,X_L,Y_L,cx,cy,xmin_new,ymin_new,bbox_side)\n","    [X_M,Y_M] = rotate_and_normalize_landmarks(R,X_M,Y_M,cx,cy,xmin_new,ymin_new,bbox_side)\n","    \n","    #We will use the last entry of this vector to interleave the dataset\n","    output_data = [X_A, Y_A, X_B, Y_B, X_C, Y_C, X_D, Y_D, X_E,Y_E,X_F,Y_F, X_G, Y_G, X_H, Y_H, X_I, Y_I, X_L, Y_L, X_M, Y_M, dataset_class]\n","    output_data = tf.reshape(output_data,[23])\n","\n","    #Further augmentations: edit the function pixel_level_augment\n","    #Augment only synthetic images\n","    image = tf.cond(tf.equal(dataset_class,1.0), lambda: pixel_level_augment(image,target_image_size), lambda: image)\n","\n","    #Augment all images\n","    #image = pixel_level_augment(image,target_image_size)\n","\n","    image = tf.clip_by_value(image,0,255)\n","    \n","    #To RGB\n","    image=tf.image.grayscale_to_rgb(image)\n","\n","    #Rescale\n","    image = (image / 127.5) - 1.0\n","\n","    image = tf.reshape(image, [target_image_size, target_image_size, 3])\n","\n","    #We make no difference here between sunlamp and lightbox, we consider only synthetic / real\n","    dataset_class=tf.cond(tf.equal(dataset_class,1.0), lambda: 0.0, lambda: 1.0)\n","\n","    return image, {'discriminator': dataset_class, 'regressor': output_data}\n","\n","def rotate_and_normalize_landmarks(R,xp,yp,cx,cy,xmin_new, ymin_new,bbox_side):\n","\n","    q=tf.tensordot(R,tf.stack([xp,yp]),axes=1)\n","    xp=q[0]+cx-xmin_new\n","    yp=q[1]+cy-ymin_new\n","\n","    xpn=xp/bbox_side\n","    ypn=yp/bbox_side\n","    \n","    return xpn, ypn\n","\n","def scale_and_translate(xmin_rotated,xmax_rotated,ymin_rotated,ymax_rotated,bbox_side_old,x_c,y_c,target_image_size,image_height,image_width):\n","\n","    #Random bbox scaling factor 0-15%. In case the bounding box is already as large as the image height we avoid increasing it further\n","    #also, the maximum scaling factor should not lead to a bbox greater than the image height.\n","    #If bbox size il equal to image height, ymin is already at 0 and ymax is at image height\n","    ss = tf.random.uniform( shape=[], minval=1.00, maxval=tf.minimum(image_height/bbox_side_old,1.15),seed=1089 )\n","    scaling_factor=tf.cond(tf.equal(bbox_side_old,image_height), lambda:1.00, lambda: ss)\n","      \n","    bbox_side_new=bbox_side_old*scaling_factor\n","    \n","    #Enlarge bbox if smaller than target image size\n","    bbox_side_new=tf.cond(tf.less(bbox_side_new,target_image_size),lambda: tf.constant(target_image_size,dtype=tf.float32),lambda: bbox_side_new)\n","\n","    #Default translation is between -0.1 and +0.1, but we limit shifting values to ensure xmin and xmax are within the image dimensions\n","    h_default=tf.random.uniform(shape=[], minval=tf.reduce_max([-0.1,0.5-x_c/bbox_side_new,(xmax_rotated-x_c)/bbox_side_new-0.5]),\n","                                                                          maxval=tf.reduce_min([0.1, (image_width-x_c)/bbox_side_new-0.5,(xmin_rotated-x_c)/bbox_side_new+0.5]),seed=15)\n","\n","    h_shift_factor=tf.case([(tf.equal(scaling_factor,1.00),lambda:0.0),\n","                              (tf.equal(xmin_rotated,0.0),lambda: 0.5-x_c/bbox_side_new),\n","                              (tf.equal(xmax_rotated,image_width),lambda: (image_width-x_c)/bbox_side_new-0.5)],\n","                              default=lambda: h_default)\n","    \n","    v_default=tf.random.uniform(shape=[], minval=tf.reduce_max([-0.1,0.5-y_c/bbox_side_new,(ymax_rotated-y_c)/bbox_side_new-0.5]),\n","                                                                          maxval=tf.reduce_min([0.1, (image_height-y_c)/bbox_side_new-0.5,(ymin_rotated-y_c)/bbox_side_new+0.5]),seed=24)\n","\n","    v_shift_factor=tf.case([(tf.logical_and(tf.equal(ymax_rotated,image_height),tf.equal(ymin_rotated,0.0)),lambda: 0.0),\n","                              (tf.equal(ymin_rotated,0.0),lambda: 0.5-y_c/bbox_side_new),\n","                              (tf.equal(ymax_rotated,image_height),lambda: (image_height-y_c)/bbox_side_new-0.5)],\n","                              default=lambda: v_default)\n","                              \n","    xmin_new=(x_c-bbox_side_new/2.0+h_shift_factor*bbox_side_new)\n","    ymin_new=(y_c-bbox_side_new/2.0+v_shift_factor*bbox_side_new)\n","    xmax_new=(x_c+bbox_side_new/2.0+h_shift_factor*bbox_side_new)\n","    ymax_new=(y_c+bbox_side_new/2.0+v_shift_factor*bbox_side_new)\n","    \n","    xmin_new=tf.maximum(xmin_new,0.0)\n","    ymin_new=tf.maximum(ymin_new,0.0)\n","    \n","    return xmin_new, ymin_new, bbox_side_new\n","\n","\n","def pixel_level_augment(image,target_image_size):\n","    prob_brightness = tf.random.uniform([],minval=0,maxval=1,seed=432)\n","    image = tf.cond(tf.less(prob_brightness,0.5), lambda: tf.image.random_brightness(image, max_delta=0.2,seed=1), lambda: image)\n","\n","    prob_contrast = tf.random.uniform([],minval=0,maxval=1,seed=6736)\n","    image = tf.cond(tf.less(prob_contrast,0.5), lambda: tf.image.random_contrast(image,0.1,1.5,seed=2), lambda: image)\n","\n","    prob_blur = tf.random.uniform([],minval=0,maxval=1,seed=787)\n","    image = tf.cond(tf.less(prob_blur,0.5), lambda: tfa.image.gaussian_filter2d(image, sigma=1), lambda: image)\n","\n","    prob_noise = tf.random.uniform([],minval=0,maxval=1,seed=782)\n","    image = tf.cond(tf.less(prob_noise,0.5), lambda: add_gauss_noise(image,target_image_size), lambda: image)\n","\n","    return image\n","\n","def add_gauss_noise(image,target_image_size):\n","      \n","      mean = 0\n","      std = 0.047\n","      \n","      gauss = tf.random.normal([target_image_size,target_image_size,1], mean,std,seed=957)\n","\n","      noisy = image + gauss\n","      return noisy\n","\n","def map_validation_dataset(image, features, target_image_size):\n","\n","    dataset_class=tf.cast(features['image/dataset_class'],dtype=tf.float32)\n","    image_height=tf.cast(features['image/height'],dtype=tf.float32)\n","    image_width=tf.cast(features['image/width'],dtype=tf.float32)\n","\n","    xmin=features['image/object/bbox/xmin']\n","    ymin=features['image/object/bbox/ymin']\n","    xmax=features['image/object/bbox/xmax']\n","    ymax=features['image/object/bbox/ymax']\n","\n","    #Get bbox center\n","    xc=(xmax+xmin)/2\n","    yc=(ymax+ymin)/2\n","\n","    #Enlarge bbox by 15%\n","    bbox_w=tf.reduce_max([(xmax-xmin)*1.15,target_image_size])\n","    bbox_h=tf.reduce_max([(ymax-ymin)*1.15, target_image_size])\n","\n","    #Get bbox upper vertex\n","    xmin=xc-bbox_w/2\n","    ymin=yc-bbox_h/2\n","    \n","    #Clip to image borders\n","    xmin=tf.reduce_max([xmin,0])\n","    ymin=tf.reduce_max([ymin,0])\n","\n","    xmin=tf.math.floor(xmin)\n","    ymin=tf.math.floor(ymin)\n","\n","    bbox_h=tf.reduce_min([bbox_h,image_height-ymin])\n","    bbox_w=tf.reduce_min([bbox_w,image_width-xmin])\n","    bbox_h=tf.math.floor(bbox_h)\n","    bbox_w=tf.math.floor(bbox_w)\n","\n","    cropped_img = tf.image.crop_to_bounding_box(image,\n","                                        tf.cast(ymin,tf.int32),\n","                                        tf.cast(xmin,tf.int32),\n","                                        tf.cast(bbox_h,tf.int32),\n","                                        tf.cast(bbox_w,tf.int32),\n","                                        )\n","    \n","    cropped_img_shape = tf.shape(cropped_img);\n","\n","    rows=tf.cast(cropped_img_shape[0],tf.int32)\n","    cols=tf.cast(cropped_img_shape[1],tf.int32)\n","\n","    [cropped_img,xmin,ymin] = tf.cond(tf.math.less(rows,cols), lambda: pad_rows(cropped_img,cols,rows,xmin,ymin), lambda: pad_cols(cropped_img,cols,rows,xmin,ymin))\n","    \n","    image=tf.image.resize(cropped_img,\n","                          [target_image_size,target_image_size],\n","                          method=tf.image.ResizeMethod.BILINEAR,\n","                          antialias=False\n","    )\n","\n","    image= tf.image.grayscale_to_rgb(image)\n","    \n","    image = tf.reshape(image, [target_image_size, target_image_size,3])\n","\n","    bbox_side=tf.cast(tf.reduce_max(cropped_img_shape),tf.float32)\n","    xmin=tf.cast(xmin,tf.float32)\n","    ymin=tf.cast(ymin,tf.float32)\n","\n","    X_A=(features['image/object/kpts/X_A']-xmin)/bbox_side\n","    Y_A=(features['image/object/kpts/Y_A']-ymin)/bbox_side\n","    X_B=(features['image/object/kpts/X_B']-xmin)/bbox_side\n","    Y_B=(features['image/object/kpts/Y_B']-ymin)/bbox_side\n","    X_C=(features['image/object/kpts/X_C']-xmin)/bbox_side\n","    Y_C=(features['image/object/kpts/Y_C']-ymin)/bbox_side\n","    X_D=(features['image/object/kpts/X_D']-xmin)/bbox_side\n","    Y_D=(features['image/object/kpts/Y_D']-ymin)/bbox_side\n","    X_E=(features['image/object/kpts/X_E']-xmin)/bbox_side\n","    Y_E=(features['image/object/kpts/Y_E']-ymin)/bbox_side\n","    X_F=(features['image/object/kpts/X_F']-xmin)/bbox_side\n","    Y_F=(features['image/object/kpts/Y_F']-ymin)/bbox_side\n","    X_G=(features['image/object/kpts/X_G']-xmin)/bbox_side\n","    Y_G=(features['image/object/kpts/Y_G']-ymin)/bbox_side\n","    X_H=(features['image/object/kpts/X_H']-xmin)/bbox_side\n","    Y_H=(features['image/object/kpts/Y_H']-ymin)/bbox_side\n","    X_I=(features['image/object/kpts/X_I']-xmin)/bbox_side\n","    Y_I=(features['image/object/kpts/Y_I']-ymin)/bbox_side\n","    X_L=(features['image/object/kpts/X_L']-xmin)/bbox_side\n","    Y_L=(features['image/object/kpts/Y_L']-ymin)/bbox_side\n","    X_M=(features['image/object/kpts/X_M']-xmin)/bbox_side\n","    Y_M=(features['image/object/kpts/Y_M']-ymin)/bbox_side\n","\n","    cx=image_width/2.0\n","    cy=image_height/2.0\n","    \n","\n","    output_data = [X_A, Y_A, X_B, Y_B, X_C, Y_C, X_D, Y_D, X_E,Y_E,X_F,Y_F, X_G, Y_G, X_H, Y_H, X_I, Y_I, X_L, Y_L, X_M, Y_M]\n","    \n","    image = (image / 127.5) - 1.0\n","\n","    dataset_class=tf.cond(tf.equal(dataset_class,1.0), lambda: 0.0, lambda: 1.0)\n","    \n","    return image, {'discriminator': dataset_class, 'regressor': output_data}\n","\n","def pad_rows(cropped_img,cols,rows,xmin,ymin):\n","    rows_to_pad_up=tf.cast((cols-rows)/2,tf.int32)\n","    padding_up=tf.zeros([rows_to_pad_up,cols,1],dtype=tf.uint8)\n","\n","    rows_to_pad_down=cols-rows-rows_to_pad_up\n","    padding_down=tf.zeros([(rows_to_pad_down),cols,1],dtype=tf.uint8)\n","\n","    cropped_img=tf.concat((padding_up,cropped_img,padding_down),axis=0)\n","    ymin = ymin-tf.cast(rows_to_pad_up,tf.float32)\n","\n","    return cropped_img, xmin,ymin\n","\n","def pad_cols(cropped_img,cols,rows,xmin,ymin):\n","    cols_to_pad_left=tf.cast((rows-cols)/2,tf.int32)\n","    padding_left=tf.zeros([rows,cols_to_pad_left,1],dtype=tf.uint8)\n","    cols_to_pad_right=rows-cols-cols_to_pad_left\n","    padding_right=tf.zeros([rows,cols_to_pad_right,1],dtype=tf.uint8)\n","\n","    cropped_img=tf.concat((padding_left,cropped_img,padding_right),axis=1)\n","    xmin = xmin-tf.cast(cols_to_pad_left,tf.float32)\n","\n","    return cropped_img,xmin,ymin"],"metadata":{"id":"p9R-AdHHDJ6Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# (Optional) Visualize the dataset\n","Use the following cells to visualize the dataset."],"metadata":{"id":"8MNGt9jSFtYR"}},{"cell_type":"code","source":["image_size = 224\n","AUTO=tf.data.AUTOTUNE\n","\n","# Train dataset preparation\n","\n","all_data_record=load_tf_records(train_dataset_path).map(lambda x: decode_dataset(x, image_size), num_parallel_calls=AUTO,deterministic=False).map(lambda x,y: apply_augmentations(x,y,image_size),num_parallel_calls=AUTO,deterministic=False)\n","\n","# Validation dataset preparation\n","validation_dataset=load_tf_records(validation_dataset_path).map(lambda x: decode_dataset(x, image_size), num_parallel_calls=AUTO,deterministic=False).map(lambda x, y: map_validation_dataset(x,y, image_size), num_parallel_calls=AUTO)\n"],"metadata":{"id":"iVVSQYKoiM1Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import numpy as np\n","even=np.arange(0,22,2)\n","odd=np.arange(1,22,2)\n","\n","for image, label in all_data_record.take(10):\n","  \n","  label1=np.reshape(label['regressor'][:-1],22)*image_size\n","\n","  plt.imshow((image+1)*0.5)\n","\n","  plt.plot(label1[even],label1[odd],'.')\n","  plt.show()\n","\n","  print(label)"],"metadata":{"id":"AYvkW59TiikY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training pipeline\n","Dataset preprocessing."],"metadata":{"id":"5PSQdc_iEHPg"}},{"cell_type":"code","source":["image_size = input_shape\n","AUTO=tf.data.AUTOTUNE\n","batch_size=48\n","epochs = 40\n","\n","# Train dataset preparation\n","\n","all_data_record=load_tf_records(train_dataset_path).map(lambda x: decode_dataset(x, image_size), num_parallel_calls=AUTO,deterministic=False).cache().map(lambda x,y: apply_augmentations(x,y,image_size),num_parallel_calls=AUTO,deterministic=False)\n","\n","@tf.function()\n","def get_synthetic(ds):\n","  return ds.filter(lambda x, y: tf.equal(y['regressor'][-1],1.))\n","\n","@tf.function()\n","def get_lightbox(ds):\n","  return ds.filter(lambda x, y: tf.equal(y['regressor'][-1],2.))\n","\n","@tf.function()\n","def get_sunlamp(ds):\n","  return ds.filter(lambda x, y: tf.equal(y['regressor'][-1],3.))\n","\n","synthetic_ds = get_synthetic(all_data_record).repeat().take(15992*3) #few images repeated\n","lightbox_ds = get_lightbox(all_data_record).repeat().take(23988) # Repeat as many time as needed and take 15992*3/2\n","sunlamp_ds = get_sunlamp(all_data_record).repeat().take(23988) # Repeat as many time as needed and take 15992*3/2\n","\n","real_ds = lightbox_ds.concatenate(sunlamp_ds).shuffle(1000, seed=1) # Small shuffle buffer size for performance reasons\n","\n","# Batch the datasets for correct interleaving ratio:\n","synthetic_ds_batched = synthetic_ds.batch(3)\n","real_ds_batched = real_ds.batch(3)\n","\n","# Populate batch 50-50 synthetic-real\n","train_ds=tf.data.Dataset.zip((synthetic_ds_batched, real_ds_batched)).map(lambda x, y: (tf.concat((x[0], y[0]), axis=0), {'discriminator': tf.concat((x[1]['discriminator'],y[1]['discriminator']),axis=0), 'regressor': tf.concat((x[1]['regressor'],y[1]['regressor']),axis=0)}),num_parallel_calls=AUTO).unbatch().prefetch(AUTO)\n","\n","train_dataset = train_ds.batch(batch_size,drop_remainder=True).repeat()\n","\n","# Validation dataset\n","validation_dataset=load_tf_records(validation_dataset_path).map(lambda x: decode_dataset(x, image_size), num_parallel_calls=AUTO,deterministic=False).map(lambda x, y: map_validation_dataset(x,y, image_size), num_parallel_calls=AUTO).batch(batch_size,drop_remainder=True).cache().repeat().prefetch(AUTO)\n","\n","# Distribute datasets on TPU\n","train_ds_distributed=tpu_strategy.experimental_distribute_dataset(train_dataset)\n","valid_ds_distributed=tpu_strategy.experimental_distribute_dataset(validation_dataset)\n","\n","dataset_size = 15992*3*2\n","validation_size = 11994\n","\n","steps_per_epoch=dataset_size//batch_size\n","validation_steps=validation_size//batch_size\n","\n","# Create train dataset iterator\n","train_iterator = iter(train_ds_distributed)\n","valid_iterator = iter(valid_ds_distributed)\n"],"metadata":{"id":"HmVyPK8cQFyR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define losses and metrics."],"metadata":{"id":"7bgaQgt1PcFW"}},{"cell_type":"code","source":["GLOBAL_BATCH_SIZE = batch_size\n","synthetic_images_per_replica = 3 # With global batch size 48 and 6 batch size per replica\n","\n","### Losses:\n","with tpu_strategy.scope():\n","  # initialize cls loss with no reduction, reference: https://www.tensorflow.org/tutorials/distribute/custom_training#define_the_loss_function\n","  # from logits = True when we do not apply a sigmoid function to the output dense layer\n","  cls_lossobj = tf.keras.losses.BinaryCrossentropy(\n","      reduction=tf.keras.losses.Reduction.NONE,\n","      from_logits=True)\n","\n","  def discriminator_loss(labels, predictions):\n","    per_example_loss = cls_lossobj(labels, predictions)\n","    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=batch_size)\n","\n","  def mae_loss(y_true,y_pred):\n","    # We know that the first three elements of the batch (axis 0) are the synthetic images.\n","    #y_true as also contains the dataset_class, used to interleave synthetic and real images\n","    y_true = y_true[0:3,0:num_keypoints*2]\n","    y_pred = y_pred[0:3,0:num_keypoints*2]\n","\n","    # Compute the absolute error for each coordinate and take the average\n","    per_example_loss = tf.math.reduce_mean(tf.math.abs(y_pred-y_true),axis=-1)\n","\n","    synthetic_images_per_batch = synthetic_images_per_replica*8\n","\n","    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=synthetic_images_per_batch)\n","\n","  def val_mae_loss(y_true,y_pred):\n","    \n","    # Compute the absolute error for each coordinate and take the average \n","    per_example_loss = tf.math.reduce_mean(tf.math.abs(y_pred-y_true),axis=-1)\n","\n","    # In the validation loss we average over the entire batch since all images are synthetic\n","    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n","\n","### Metrics\n","with tpu_strategy.scope(): \n","  \n","  # train loss accumulators\n","  encoder_loss_tracker = tf.keras.metrics.Mean(name=\"encoder_loss\")\n","  bbox_loss_tracker = tf.keras.metrics.Mean(name=\"bbox_loss\")\n","  cls_loss_tracker = tf.keras.metrics.Mean(name=\"cls_loss\")\n","\n","  # validation loss accumulators\n","  encoder_val_loss_tracker = tf.keras.metrics.Mean(name=\"encoder_val_loss\")\n","  bbox_val_loss_tracker = tf.keras.metrics.Mean(name=\"bbox_val_loss\")\n","  cls_val_loss_tracker = tf.keras.metrics.Mean(name=\"cls_val_loss\")\n","\n","  # Binary accuracy references: https://www.tensorflow.org/hub/tutorials/tf2_text_classification and https://github.com/tensorflow/tensorflow/issues/41413\n","\n","  # Threshold = 0 when from logits True in loss\n","  cls_accuracy_tracker = tf.keras.metrics.BinaryAccuracy(name='cls_accuracy', threshold=0.0)\n","  cls_val_accuracy_tracker = tf.keras.metrics.BinaryAccuracy(name='cls_val_accuracy', threshold=0.0)"],"metadata":{"id":"9gs_Ef-nNPK3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Optimizers and checkpoint manager."],"metadata":{"id":"qn98N9o6QfMN"}},{"cell_type":"code","source":["import time\n","K = tf.keras.backend \n","import tensorflow_addons as tfa\n","\n","logger=tf.get_logger()\n","\n","total_steps = steps_per_epoch*epochs\n","\n","with tpu_strategy.scope(): \n","  \n","  # gamma value to be used for first epoch\n","  gamma = K.variable(0.0)\n","\n","  # We define an optimizer for each NN element\n","  optimizer_encoder=tfa.optimizers.AdamW(weight_decay=1e-8,\n","      learning_rate=tf.keras.optimizers.schedules.CosineDecay(5e-5, total_steps)\n","  )\n","  \n","  optimizer_regressor=tfa.optimizers.AdamW(weight_decay=1e-8,\n","      learning_rate=tf.keras.optimizers.schedules.CosineDecay(5e-5, total_steps)\n","  )\n","\n","  optimizer_discriminator=tfa.optimizers.AdamW(weight_decay=1e-8,\n","      learning_rate=tf.keras.optimizers.schedules.CosineDecay(5e-5, total_steps)\n","  )\n","\n","  # Checkpoint manager\n","  checkpoint = tf.train.Checkpoint(\n","      epoch=tf.Variable(-1), # we add 1 as soon as we start training\n","      optimizer_encoder=optimizer_encoder,\n","      optimizer_regressor=optimizer_regressor,\n","      optimizer_discriminator=optimizer_discriminator,\n","      network=network,\n","      )\n","  \n","  manager = tf.train.CheckpointManager(checkpoint,checkpoint_prefix, max_to_keep=3)"],"metadata":{"id":"KxpR2bPKQiTV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Custom training loop: note, the code is configured for Swin Transformers backbones. To train the EfficientNet based model, modify the \"index\" variables in the cell below."],"metadata":{"id":"OyMWON2RQkKL"}},{"cell_type":"code","source":["@tf.function\n","def train_step(iterator,steps_per_epoch):\n","  \"\"\"The step function for one training step.\"\"\"\n","  def step_fn(data):\n","        images, y_true = data\n","        y_truecls = tf.reshape(y_true['discriminator'],[GLOBAL_BATCH_SIZE//8,1])\n","\n","        with tf.GradientTape(persistent=True) as tape:\n","          # Forward pass\n","          y_pred_cls, y_pred_bbox = network(images,training=True)\n","\n","          # Compute losses \n","          cls_loss = discriminator_loss(y_truecls,y_pred_cls)\n","          bbox_loss=mae_loss(y_true['regressor'],y_pred_bbox)\n","          encoder_loss = bbox_loss - cls_loss*gamma*0.01 \n","\n","        # Accumulate loss, reference: https://www.tensorflow.org/guide/tpu#improving_performance_with_multiple_steps_inside_tffunction\n","        bbox_loss_tracker.update_state(bbox_loss*tpu_strategy.num_replicas_in_sync)\n","        cls_loss_tracker.update_state(cls_loss*tpu_strategy.num_replicas_in_sync)\n","        encoder_loss_tracker.update_state(encoder_loss*tpu_strategy.num_replicas_in_sync)\n","\n","        cls_accuracy_tracker.update_state(y_truecls , y_pred_cls)\n","\n","        # Compute gradients for encoder\n","        trainable_vars_encoder = network.get_layer(index=1).trainable_variables\n","        gradients_encoder = tape.gradient(encoder_loss, trainable_vars_encoder)\n","\n","\n","        # Update weights for encoder: https://www.tensorflow.org/guide/tpu#improving_performance_with_multiple_steps_inside_tffunction\n","        optimizer_encoder.apply_gradients(list(zip(gradients_encoder, trainable_vars_encoder)))\n","\n","        # Compute gradients for discrimination head\n","        trainable_vars_discriminator = network.get_layer(index=3).trainable_variables # set index = 2 for EfficientNet\n","        gradients_discriminator = tape.gradient(cls_loss, trainable_vars_discriminator)\n","        \n","        # Update weights for discrimination head\n","        optimizer_discriminator.apply_gradients(list(zip(gradients_discriminator, trainable_vars_discriminator)))\n","\n","        # Compute gradients for regression head\n","        trainable_vars_regressor = network.get_layer(index=4).trainable_variables # set index = 3 for EfficientNet\n","        gradients_regressor = tape.gradient(bbox_loss, trainable_vars_regressor)\n","        \n","        # Update weights for regression head\n","        optimizer_regressor.apply_gradients(list(zip(gradients_regressor, trainable_vars_regressor)))\n","\n","\n","  for _ in tf.range(steps_per_epoch):\n","    tpu_strategy.run(step_fn, args=(next(iterator),))\n","\n","@tf.function\n","def valid_step(data_iter,validation_steps):\n","  def valid_step_fn(data):\n","        images, y_true = data\n","        y_truecls = tf.reshape(y_true['discriminator'],[GLOBAL_BATCH_SIZE//8,1])\n","\n","        y_pred_cls, y_pred_bbox = network(images,training=False)\n","        \n","        # Compute losses\n","        cls_val_loss = discriminator_loss(y_truecls,y_pred_cls)\n","        bbox_val_loss=val_mae_loss(y_true['regressor'],y_pred_bbox)\n","        encoder_val_loss = bbox_val_loss - cls_val_loss*gamma*0.01\n","\n","        # Accumulate losses\n","        bbox_val_loss_tracker.update_state(bbox_val_loss*tpu_strategy.num_replicas_in_sync)\n","        cls_val_loss_tracker.update_state(cls_val_loss*tpu_strategy.num_replicas_in_sync)\n","        encoder_val_loss_tracker.update_state(encoder_val_loss*tpu_strategy.num_replicas_in_sync)\n","\n","        # Compute and accumulate accuracy\n","        cls_val_accuracy_tracker.update_state(y_truecls , y_pred_cls)\n","        \n","  for _ in tf.range(validation_steps):\n","    tpu_strategy.run(valid_step_fn, args=(next(data_iter),))"],"metadata":{"id":"fMBEKkGbf3yg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def restore_model(manager):\n","  with tpu_strategy.scope(): \n","    if manager.latest_checkpoint:\n","      checkpoint.restore(manager.latest_checkpoint)\n","      print(\"Restored from {}\".format(manager.latest_checkpoint))\n","    else:\n","      print(\"Initializing from scratch.\")\n","    return checkpoint\n","\n","def train_model(eventually_restored_checkpoint,manager):    \n","  \n","  while checkpoint.epoch.numpy() +1 < epochs:\n","    epoch_start_time = time.time()\n","    eventually_restored_checkpoint.epoch.assign_add(1)\n","    epoch=eventually_restored_checkpoint.epoch.numpy()\n","    print('Epoch: {}, initial learning rate: {}'.format(epoch+1, round(optimizer_encoder._decayed_lr(tf.float32).numpy(),10)))\n","    \n","    # update gamma\n","    K.set_value(gamma, 2.0/(1.0+tf.math.exp(-10.0*(epoch+1)/epochs))-1.0) \n","\n","    # train step\n","    train_step(train_iterator,steps_per_epoch)\n","\n","    print('Current step: {}, encoder loss: {}, kpts loss: {}, cls loss: {}, cls_accuracy: {}\\n'.format(\n","      optimizer_encoder.iterations.numpy(),\n","      round(float(encoder_loss_tracker.result()), 4),\n","      round(float(bbox_loss_tracker.result()), 4),\n","      round(float(cls_loss_tracker.result()), 4),\n","      round(float(cls_accuracy_tracker.result()),4)))\n","\n","    # validation step\n","    valid_step(valid_iterator,tf.convert_to_tensor(validation_steps))\n","    print('Validation data - encoder loss: {}, kpts loss: {}, cls loss: {}, cls_accuracy: {}\\n'.format(\n","      round(float(encoder_val_loss_tracker.result()), 4),\n","      round(float(bbox_val_loss_tracker.result()), 4),\n","      round(float(cls_val_loss_tracker.result()), 4),\n","      round(float(cls_val_accuracy_tracker.result()),4)))\n","\n","    # Export checkpoints every 5 epochs\n","    if (epoch+1) % 5 == 0:\n","      print('\\n Saving checkpoint...\\n')\n","\n","      with tpu_strategy.scope():\n","        manager.save()\n","\n","    # Train accumulators reset\n","    encoder_loss_tracker.reset_states()\n","    bbox_loss_tracker.reset_states()\n","    cls_loss_tracker.reset_states()\n","    cls_accuracy_tracker.reset_states()\n","\n","    # Validation accumulators reset\n","    encoder_val_loss_tracker.reset_states()\n","    bbox_val_loss_tracker.reset_states()\n","    cls_val_loss_tracker.reset_states()\n","    cls_val_accuracy_tracker.reset_states()"],"metadata":{"id":"DdG-kkYrXIgn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Resume checkpoint."],"metadata":{"id":"cbZPgyz5lsO3"}},{"cell_type":"code","source":["### TO RESTORE FROM CHECKPOINT RUN THIS CELL BEFORE TRAINING\n","\n","del(network,optimizer_regressor,optimizer_encoder,optimizer_discriminator)\n","\n","\n","checkpoint=restore_model(manager)\n","network=checkpoint.network\n","optimizer_regressor=checkpoint.optimizer_regressor\n","optimizer_encoder=checkpoint.optimizer_encoder\n","optimizer_discriminator=checkpoint.optimizer_discriminator\n"],"metadata":{"id":"W7UMIwySqHo7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Start training and export weights."],"metadata":{"id":"aa0KtpEmlqt2"}},{"cell_type":"code","source":["train_model(checkpoint,manager)\n","network.save_weights(weights_export_dir)"],"metadata":{"id":"IUUAAGccYfDd"},"execution_count":null,"outputs":[]}]}