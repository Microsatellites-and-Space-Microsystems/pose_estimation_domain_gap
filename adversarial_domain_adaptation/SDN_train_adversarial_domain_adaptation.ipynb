{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SDN_train_adversarial_domain_adaptation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["# Adversarial Training SDNs"],"metadata":{"id":"NAe1RfDGEMGw"}},{"cell_type":"markdown","source":["This code illustrates our method for adversarial training of SDNs. The notebook is configured for running on a TPU hosted runtime on Google Colab."],"metadata":{"id":"a_Br-WZexRMu"}},{"cell_type":"markdown","source":["# Preliminaries"],"metadata":{"id":"cfsliSyQxVxT"}},{"cell_type":"markdown","source":["Install required packages."],"metadata":{"id":"9WxxtEPTIlw7"}},{"cell_type":"code","source":["!pip install git+https://github.com/Microsatellites-and-Space-Microsystems/pose_estimation_domain_gap --quiet"],"metadata":{"id":"rVzntdpLIiIj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Provide access to Google Drive."],"metadata":{"id":"FmZBp_YvxZ1s"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"kinntVwjxZB7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Set network name and dataset directories."],"metadata":{"id":"tBwAS1w9xfaU"}},{"cell_type":"code","source":["import os\n","\n","network_name='my_first_SDN'\n","\n","#Directories to train and validation datasets\n","train_dataset_path='gs://.../*.record'\n","validation_dataset_path='gs://.../*.record'\n","\n","#Directory for saving trained weights\n","google_drive_base_dir='/content/gdrive/MyDrive/'\n","weights_export_dir=google_drive_base_dir+network_name+'.h5'\n","\n","#Directory for checkpoints\n","checkpoint_dir = 'gs://.../'+network_name+'/training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"],"metadata":{"id":"hOrJZedGxkXE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Set seeds."],"metadata":{"id":"3feVsC1i-Mgb"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import random as rnd\n","\n","rnd.seed(242)\n","np.random.seed(312)\n","tf.random.set_seed(112)"],"metadata":{"id":"yy2af5ZF-OhF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Initialize the TPU."],"metadata":{"id":"NfWQIZqcxr7E"}},{"cell_type":"code","source":["try:\n","  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n","            \n","  print('Connection to TPU server successfull!')\n","            \n","except ValueError:\n","  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n","\n","tf.config.experimental_connect_to_cluster(tpu)\n","tf.tpu.experimental.initialize_tpu_system(tpu)\n","tpu_strategy = tf.distribute.TPUStrategy(tpu)"],"metadata":{"id":"PDK2IOWjO07I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To train the NNs with Cloud TPUs, the dataset must be stored in a Cloud Bucket. Then it is necessary to give the TPU access to the Bucket."],"metadata":{"id":"CgWRaQ-uxwFt"}},{"cell_type":"code","source":["#A convinent way to provide access to Google Cloud Platform is to create a service account https://cloud.google.com/iam/docs/creating-managing-service-account-keys#iam-service-account-keys-create-console linked to the project\n","#The procedure will download a .json file \n","#Replace the fields below with the information contained in the file\n","\n","#If using TPU, it is also necessary to enable the TPU service account (service-[project_number]@cloud-tpu.iam.gserviceaccount.com) as an IAM user for the project\n","\n","import json\n","\n","data_all={\n","  \"type\": \"service_account\",\n","  \"project_id\": ,\n","  \"private_key_id\": ,\n","  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n...==\\n-----END PRIVATE KEY-----\\n\",\n","  \"client_email\": \"\",\n","  \"client_id\": \"\",\n","  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n","  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n","  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n","  \"client_x509_cert_url\": \"\"\n","}\n","\n","parsed = json.dumps(data_all)\n","\n","with open('/content/.config/application_default_credentials.json', 'w') as f:\n","  f.write(parsed)\n","!gcloud auth activate-service-account --key-file '/content/.config/application_default_credentials.json'\n","\n","#Alternatively\n","\n","#!gcloud auth login\n","#!gcloud config set project 'myproject' #set the project id here\n","\n","#from google.colab import auth\n","#auth.authenticate_user()"],"metadata":{"id":"dyUXcW9COvcu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Initialize SDN Swin based"],"metadata":{"id":"wh9zgDaj2SrA"}},{"cell_type":"markdown","source":["Initialize the NN encoder. To try different backbones modify the imported model in the first line of the following cell."],"metadata":{"id":"XNMirrjWx36l"}},{"cell_type":"code","source":["from models_and_layers.tfswin import SwinTransformerTiny224 as transformerEncoder\n","import tensorflow as tf\n","\n","with tpu_strategy.scope(): \n","\n","  def get_encoder(input_shape):\n","\n","    input = tf.keras.layers.Input(shape=(input_shape, input_shape, 3))\n","    model=transformerEncoder(include_top=False)(input)\n","    x = tf.keras.layers.GlobalAveragePooling2D(name='avg_pool')(model)\n","    model = tf.keras.models.Model(inputs=input, outputs=x)\n","    \n","    return model"],"metadata":{"id":"JOzkVcuJ2YEc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Initialize discriminator and regressor heads."],"metadata":{"id":"USaTy1J-yAZc"}},{"cell_type":"code","source":["class Discriminator(tf.keras.Model):\n","  def __init__(self,hidden_dim):\n","        super().__init__()\n","        \n","        self.hidden_dim = hidden_dim\n","        self.basic_layers = tf.keras.Sequential(\n","          [tf.keras.layers.Dense(self.hidden_dim*4,activation='gelu',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=422)),\n","          tf.keras.layers.Dense(self.hidden_dim,activation='gelu',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=342)),\n","          tf.keras.layers.Dense(1,name='cls',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=324)),\n","          ]\n","        )\n","  def call(self, x):\n","    x = self.basic_layers(x)\n","    return x\n","  \n","class Regressor(tf.keras.Model):\n","  def __init__(self,hidden_dim):\n","        super().__init__()\n","        \n","        self.hidden_dim = hidden_dim\n","        self.basic_layers = tf.keras.Sequential(\n","        [tf.keras.layers.Dense(hidden_dim*4,activation='gelu',name='bbox1',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=3256)),\n","         tf.keras.layers.Dense(hidden_dim,activation='gelu',name='bbox2',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=768)),\n","         tf.keras.layers.Dense(4,activation='linear',name='bbox',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=4236)),\n","         ]\n","        )\n","  def call(self, x):\n","    x = self.basic_layers(x)\n","    return x"],"metadata":{"id":"PuB8QpAonj7S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Build the model."],"metadata":{"id":"2_tA1cECyoF8"}},{"cell_type":"code","source":["# Build the model:\n","\n","input_shape = 224\n","hidden_dim=768\n","\n","with tpu_strategy.scope(): \n","  encoder=get_encoder(input_shape)\n","  discriminator = Discriminator(hidden_dim)(encoder.output)\n","  regressor=Regressor(hidden_dim)(encoder.output)\n","  network=tf.keras.models.Model([encoder.input], [discriminator,regressor])\n"],"metadata":{"id":"pmCAmUZ5RAWt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualize NN details."],"metadata":{"id":"YoChEi87yuOb"}},{"cell_type":"code","source":["network.summary()"],"metadata":{"id":"OGNHcSHDRiQt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.keras.utils.plot_model(network,show_shapes=True)"],"metadata":{"id":"XpXaXzZ7SBTg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Initialize SDN EfficientNet based"],"metadata":{"id":"0xFxA1z0yxl7"}},{"cell_type":"code","source":["from models_and_layers.efficientnet import EfficientNetV1B5\n","\n","class get_encoder(tf.keras.Model):\n","  def __init__(self,hidden_dim,input_shape):\n","        super().__init__()\n","        \n","        self.hidden_dim = hidden_dim\n","        self.basic_layers = tf.keras.Sequential([\n","    EfficientNetV1B5(num_classes=0,input_shape=(input_shape,input_shape,3),pretrained=\"imagenet\"),\n","    tf.keras.layers.Conv2D(self.hidden_dim,1,kernel_initializer=tf.keras.initializers.GlorotUniform(seed=927)),\n","    tf.keras.layers.GlobalAveragePooling2D(name='avg_pool')])\n","         \n","  def call(self, x):\n","    x = self.basic_layers(x)\n","    return x"],"metadata":{"id":"qEEtRaMqzdEK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Discriminator(tf.keras.Model):\n","  def __init__(self,hidden_dim):\n","        super().__init__()\n","        \n","        self.hidden_dim = hidden_dim\n","        self.basic_layers = tf.keras.Sequential(\n","          [tf.keras.layers.Dense(self.hidden_dim*4,activation='gelu',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=422)),\n","          tf.keras.layers.Dense(self.hidden_dim,activation='gelu',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=342)),\n","          tf.keras.layers.Dense(1,name='cls',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=324)),\n","          ]\n","        )\n","  def call(self, x):\n","    x = self.basic_layers(x)\n","    return x\n","  \n","class Regressor(tf.keras.Model):\n","  def __init__(self,hidden_dim):\n","        super().__init__()\n","        \n","        self.hidden_dim = hidden_dim\n","        self.basic_layers = tf.keras.Sequential(\n","        [tf.keras.layers.Dense(hidden_dim*4,activation='gelu',name='bbox1',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=3256)),\n","         tf.keras.layers.Dense(hidden_dim,activation='gelu',name='bbox2',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=768)),\n","         tf.keras.layers.Dense(4,activation='linear',name='bbox',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=4236)),\n","         ]\n","        )\n","  def call(self, x):\n","    x = self.basic_layers(x)\n","    return x"],"metadata":{"id":"Yv4U5t1DzeRr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build the model:\n","\n","input_shape = 224  #Assumed squared\n","hidden_dim=768     #Output size of the encoder\n","input=tf.keras.layers.Input(shape=(input_shape, input_shape, 3))\n","\n","with tpu_strategy.scope(): \n","  encoder=get_encoder(hidden_dim,input_shape)\n","  discriminator = Discriminator(hidden_dim)(encoder(input))\n","  regressor=Regressor(hidden_dim)(encoder(input))\n","  network=tf.keras.models.Model([encoder.input], [discriminator,regressor])\n"],"metadata":{"id":"X3sq0Ecjzhuy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualize NN details."],"metadata":{"id":"myOrIKgpznSS"}},{"cell_type":"code","source":["network.summary()"],"metadata":{"id":"p0N6NDekzmwi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.keras.utils.plot_model(network,show_shapes=True)"],"metadata":{"id":"1L9J5DPZzpfC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset Preprocessing"],"metadata":{"id":"5PSQdc_iEHPg"}},{"cell_type":"code","source":["import tensorflow_addons as tfa\n","\n","#Load TFRecords files\n","\n","def load_tf_records(filepath):\n","    ignore_order = tf.data.Options()\n","    ignore_order.experimental_deterministic = False\n","\n","    filenames = tf.io.gfile.glob(filepath)\n","    dataset = tf.data.TFRecordDataset(filenames,num_parallel_reads=tf.data.experimental.AUTOTUNE)\n","    dataset = dataset.with_options(ignore_order)\n","    \n","    return dataset\n","\n","#Define TFRecord structure\n","\n","def tf_records_file_features_description():\n","    image_feature_description = {\n","        'image/actual_channels': tf.io.FixedLenFeature([], tf.int64),\n","        'image/height': tf.io.FixedLenFeature([], tf.int64),\n","        'image/dataset_class': tf.io.FixedLenFeature([], tf.int64),\n","        'image/width': tf.io.FixedLenFeature([], tf.int64),\n","        'image/filename': tf.io.FixedLenFeature([], tf.string),\n","        \n","        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n","        'image/format': tf.io.FixedLenFeature([], tf.string),\n","\n","        'image/object/bbox/xmin':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/bbox/xmax':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/bbox/ymin':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/bbox/ymax':tf.io.FixedLenFeature([], tf.float32),\n","\n","        'image/object/kpts/X_A':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/Y_A':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/X_B':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/Y_B':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/X_C':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/Y_C':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/X_D':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/Y_D':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/X_E':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/Y_E':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/X_F':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/Y_F':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/X_G':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/Y_G':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/X_H':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/Y_H':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/X_I':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/Y_I':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/X_L':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/Y_L':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/X_M':tf.io.FixedLenFeature([], tf.float32),\n","        'image/object/kpts/Y_M':tf.io.FixedLenFeature([], tf.float32),\n","    }\n","    return image_feature_description\n","\n","#Decode JPEG and resize (we will cache the output)\n","\n","def decode_dataset(example_proto,image_size):\n","    features=tf.io.parse_single_example(example_proto, tf_records_file_features_description())\n","    \n","    raw_image = tf.io.decode_jpeg(features['image/encoded'],channels=0) #0: Use the number of channels in the JPEG-encoded image.\n","    image=tf.image.resize(raw_image,\n","                          [image_size,image_size],\n","                          method=tf.image.ResizeMethod.BILINEAR,\n","                          antialias=False\n","    )\n","    \n","    return image, features\n","\n","#Apply augmentations during training\n","\n","def apply_augmentations(raw_image, features,target_image_size):\n","\n","    #Recover image features\n","    image_height=tf.cast(features['image/height'],dtype=tf.float32)\n","    image_width=tf.cast(features['image/width'],dtype=tf.float32)\n","    \n","    dataset_class=tf.cast(features['image/dataset_class'],dtype=tf.float32)\n","\n","    xmin=features['image/object/bbox/xmin']*target_image_size/image_width\n","    ymin=features['image/object/bbox/ymin']*target_image_size/image_height\n","    xmax=features['image/object/bbox/xmax']*target_image_size/image_width\n","    ymax=features['image/object/bbox/ymax']*target_image_size/image_height\n","\n","    #Principal point\n","\n","    cx = target_image_size/2.0\n","    cy = target_image_size/2.0\n","\n","    #The image will be rotated wrt the center point; coordinates are also re-scaled\n","    X_A=features['image/object/kpts/X_A']*target_image_size/image_width-cx\n","    Y_A=features['image/object/kpts/Y_A']*target_image_size/image_height-cy\n","    X_B=features['image/object/kpts/X_B']*target_image_size/image_width-cx\n","    Y_B=features['image/object/kpts/Y_B']*target_image_size/image_height-cy\n","    X_C=features['image/object/kpts/X_C']*target_image_size/image_width-cx\n","    Y_C=features['image/object/kpts/Y_C']*target_image_size/image_height-cy\n","    X_D=features['image/object/kpts/X_D']*target_image_size/image_width-cx\n","    Y_D=features['image/object/kpts/Y_D']*target_image_size/image_height-cy\n","    X_E=features['image/object/kpts/X_E']*target_image_size/image_width-cx\n","    Y_E=features['image/object/kpts/Y_E']*target_image_size/image_height-cy\n","    X_F=features['image/object/kpts/X_F']*target_image_size/image_width-cx\n","    Y_F=features['image/object/kpts/Y_F']*target_image_size/image_height-cy\n","    X_G=features['image/object/kpts/X_G']*target_image_size/image_width-cx\n","    Y_G=features['image/object/kpts/Y_G']*target_image_size/image_height-cy\n","    X_H=features['image/object/kpts/X_H']*target_image_size/image_width-cx\n","    Y_H=features['image/object/kpts/Y_H']*target_image_size/image_height-cy\n","    X_I=features['image/object/kpts/X_I']*target_image_size/image_width-cx\n","    Y_I=features['image/object/kpts/Y_I']*target_image_size/image_height-cy\n","    X_L=features['image/object/kpts/X_L']*target_image_size/image_width-cx\n","    Y_L=features['image/object/kpts/Y_L']*target_image_size/image_height-cy\n","    X_M=features['image/object/kpts/X_M']*target_image_size/image_width-cx\n","    Y_M=features['image/object/kpts/Y_M']*target_image_size/image_height-cy\n","    \n","    #Random rotation angle\n","    rotation_angle= tf.random.uniform(\n","        shape=[], minval=tf.constant(-np.pi), maxval=tf.constant(np.pi),seed=5000\n","    )\n","    \n","    #Rotation matrix\n","    cos = tf.cos(rotation_angle)\n","    sin = tf.sin(rotation_angle)\n","    R=tf.reshape([cos, sin, -sin,cos],[2,2])\n","    \n","    #Rotate the bounding box\n","    q=tf.matmul(R,tf.reshape([xmin-cx,xmin-cx,xmax-cx,xmax-cx,\n","                              ymin-cy,ymax-cy,ymin-cy,ymax-cy],[2,4]))\n","\n","    #tl = top left, bl = bottom left, tr = top right, br = bottom right\n","    xtl=q[0,0]\n","    ytl=q[1,0]\n","    xbl=q[0,1]\n","    ybl=q[1,1]\n","    xtr=q[0,2]\n","    ytr=q[1,2]\n","    xbr=q[0,3]\n","    ybr=q[1,3]\n","    \n","    #Recover rotated bbox coordinates in original image frame\n","    \n","    xmin_rotated=tf.reduce_min([xtl,xbl,xtr,xbr])+cx\n","    ymin_rotated=tf.reduce_min([ytl,ybl,ytr,ybr])+cy\n","\n","    #Clip the values between 0 and the original image dimensions\n","    xmin_rotated=tf.maximum(xmin_rotated,0.0)\n","    ymin_rotated=tf.maximum(ymin_rotated,0.0)\n","        \n","    #Rotation of all keypoints\n","\n","    [X_A,Y_A] = rotate_and_normalize_landmarks(R,X_A,Y_A,cx,cy,image_width,image_height,target_image_size,xmin_rotated,ymin_rotated)\n","    [X_B,Y_B] = rotate_and_normalize_landmarks(R,X_B,Y_B,cx,cy,image_width,image_height,target_image_size,xmin_rotated,ymin_rotated)\n","    [X_C,Y_C] = rotate_and_normalize_landmarks(R,X_C,Y_C,cx,cy,image_width,image_height,target_image_size,xmin_rotated,ymin_rotated)\n","    [X_D,Y_D] = rotate_and_normalize_landmarks(R,X_D,Y_D,cx,cy,image_width,image_height,target_image_size,xmin_rotated,ymin_rotated)\n","    [X_E,Y_E] = rotate_and_normalize_landmarks(R,X_E,Y_E,cx,cy,image_width,image_height,target_image_size,xmin_rotated,ymin_rotated)\n","    [X_F,Y_F] = rotate_and_normalize_landmarks(R,X_F,Y_F,cx,cy,image_width,image_height,target_image_size,xmin_rotated,ymin_rotated)\n","    [X_G,Y_G] = rotate_and_normalize_landmarks(R,X_G,Y_G,cx,cy,image_width,image_height,target_image_size,xmin_rotated,ymin_rotated)\n","    [X_H,Y_H] = rotate_and_normalize_landmarks(R,X_H,Y_H,cx,cy,image_width,image_height,target_image_size,xmin_rotated,ymin_rotated)\n","    [X_I,Y_I] = rotate_and_normalize_landmarks(R,X_I,Y_I,cx,cy,image_width,image_height,target_image_size,xmin_rotated,ymin_rotated)\n","    [X_L,Y_L] = rotate_and_normalize_landmarks(R,X_L,Y_L,cx,cy,image_width,image_height,target_image_size,xmin_rotated,ymin_rotated)\n","    [X_M,Y_M] = rotate_and_normalize_landmarks(R,X_M,Y_M,cx,cy,image_width,image_height,target_image_size,xmin_rotated,ymin_rotated)\n","\n","    xmin_rotated=tf.maximum(tf.reduce_min([X_A,X_B,X_C,X_D,X_E,X_F,X_G,X_H,X_I,X_L,X_M]),0.0)\n","    xmax_rotated=tf.minimum(tf.reduce_max([X_A,X_B,X_C,X_D,X_E,X_F,X_G,X_H,X_I,X_L,X_M]),1.0)\n","    ymin_rotated=tf.maximum(tf.reduce_min([Y_A,Y_B,Y_C,Y_D,Y_E,Y_F,Y_G,Y_H,Y_I,Y_L,Y_M]),0.0)\n","    ymax_rotated=tf.minimum(tf.reduce_max([Y_A,Y_B,Y_C,Y_D,Y_E,Y_F,Y_G,Y_H,Y_I,Y_L,Y_M]),1.0)\n","\n","    #We will use the last entry of this vector to interleave the dataset\n","    output_data = [xmin_rotated , ymin_rotated , xmax_rotated , ymax_rotated, dataset_class]\n","\n","    #Rotate image\n","    image=tfa.image.rotate(raw_image, rotation_angle)\n","\n","    #Further augmentations: edit the function pixel_level_augment\n","    #Augment only synthetic images\n","    image = tf.cond(tf.equal(dataset_class,1.0), lambda: pixel_level_augment(image,target_image_size), lambda: image)\n","\n","    #Augment all images\n","    #image = pixel_level_augment(image,target_image_size)\n","\n","    image = tf.clip_by_value(image,0,255)\n","    \n","    #To RGB\n","    image=tf.image.grayscale_to_rgb(image)\n","    \n","    #Rescale\n","    image = (image / 127.5) - 1.0\n","\n","    image = tf.reshape(image, [target_image_size, target_image_size, 3])\n","\n","    #We make no difference here between sunlamp and lightbox, we consider only synthetic / real\n","    dataset_class=tf.cond(tf.equal(dataset_class,1.0), lambda: 0.0, lambda: 1.0)\n","    \n","    return image, {'discriminator': dataset_class, 'regressor': output_data}\n","\n","def rotate_and_normalize_landmarks(R,xp,yp,cx,cy,image_width,image_height,target_image_size,xmin_rotated,ymin_rotated):\n","    \n","    q=tf.tensordot(R,tf.stack([xp,yp]),axes=1)\n","    xp=(q[0]+cx) \n","    yp=(q[1]+cy) \n","\n","    xpn=xp/target_image_size\n","    ypn=yp/target_image_size\n","    \n","    return xpn, ypn\n","\n","def pixel_level_augment(image,target_image_size):\n","    prob_brightness = tf.random.uniform([],minval=0,maxval=1,seed=271)\n","    image = tf.cond(tf.less(prob_brightness,0.5), lambda: tf.image.random_brightness(image, max_delta=0.2,seed=1), lambda: image)\n","\n","    prob_contrast = tf.random.uniform([],minval=0,maxval=1,seed=53)\n","    image = tf.cond(tf.less(prob_contrast,0.5), lambda: tf.image.random_contrast(image,0.1,1.5,seed=2), lambda: image)\n","\n","    prob_blur = tf.random.uniform([],minval=0,maxval=1,seed=857)\n","    image = tf.cond(tf.less(prob_blur,0.5), lambda: tfa.image.gaussian_filter2d(image, sigma=1), lambda: image)\n","\n","    prob_noise = tf.random.uniform([],minval=0,maxval=1,seed=738)\n","    image = tf.cond(tf.less(prob_noise,0.5), lambda: add_gauss_noise(image,target_image_size), lambda: image)\n","\n","    return image\n","\n","def add_gauss_noise(image,target_image_size):\n","      \n","      mean = 0\n","      std = 0.047\n","      \n","      gauss = tf.random.normal([target_image_size,target_image_size,1], mean,std,seed=867)\n","\n","      noisy = image + gauss\n","      return noisy\n","\n","def map_validation_dataset(image, features, target_image_size, lite_model=True):    \n","    image_height=tf.cast(features['image/height'],dtype=tf.float32)\n","    image_width=tf.cast(features['image/width'],dtype=tf.float32)\n","    dataset_class=tf.cast(features['image/dataset_class'],dtype=tf.float32)\n","\n","    xmin=features['image/object/bbox/xmin']/image_width\n","    ymin=features['image/object/bbox/ymin']/image_height\n","    xmax=features['image/object/bbox/xmax']/image_width\n","    ymax=features['image/object/bbox/ymax']/image_height\n","\n","    output_data = [xmin, ymin, xmax, ymax]\n","\n","    dataset_class=tf.cond(tf.equal(dataset_class,1.0), lambda: 0.0, lambda: 1.0)\n","\n","    image=tf.image.grayscale_to_rgb(image)\n","    image = tf.cast(image, tf.float32)\n","\n","    image = (image / 127.5) - 1.0\n","    image = tf.reshape(image, [target_image_size, target_image_size, 3])\n","\n","    return image, {'discriminator': dataset_class, 'regressor': output_data}"],"metadata":{"id":"gApHynjQz0Fj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# (Optional) Visualize the dataset"],"metadata":{"id":"ziAYPjR22It8"}},{"cell_type":"markdown","source":["Use the following cells to visualize the dataset."],"metadata":{"id":"Kf-iuuolnUwD"}},{"cell_type":"code","source":["image_size = 224\n","AUTO=tf.data.AUTOTUNE\n","\n","# Train dataset\n","\n","all_data_record=load_tf_records(train_dataset_path).map(lambda x: decode_dataset(x, image_size), num_parallel_calls=AUTO,deterministic=False).map(lambda x,y: apply_augmentations(x,y,image_size),num_parallel_calls=AUTO,deterministic=False)\n","\n","# Validation dataset\n","validation_dataset=load_tf_records(validation_dataset_path).map(lambda x: decode_dataset(x, image_size), num_parallel_calls=AUTO,deterministic=False).map(lambda x, y: map_validation_dataset(x,y, image_size), num_parallel_calls=AUTO)\n"],"metadata":{"id":"NuSnzS8H3D-g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import numpy as np\n","odd=np.arange(0,4,2)\n","even=np.arange(1,4,2)\n","\n","for image, label in all_data_record.take(10):\n","  \n","  label1=np.reshape(label['regressor'][:-1].numpy(),4)*image_size\n","\n","  fig, ax = plt.subplots()\n","\n","  ax.imshow((image+1)*0.5)\n","\n","  xmin = label1[0]\n","  ymin = label1[1]\n","  xmax = label1[2]\n","  ymax = label1[3]\n","  w = xmax-xmin\n","  h = ymax-ymin\n","  rect = patches.Rectangle((xmin, ymin), w, h, linewidth=1, edgecolor='r', facecolor='none')\n","\n","  ax.add_patch(rect)\n","  plt.show()\n","  print(label)"],"metadata":{"id":"CZdNKByp2NYy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training pipeline"],"metadata":{"id":"3yfYvEzzz1H1"}},{"cell_type":"markdown","source":["Dataset preprocessing."],"metadata":{"id":"kapBEHqS9D19"}},{"cell_type":"code","source":["image_size = input_shape\n","AUTO=tf.data.AUTOTUNE\n","batch_size=48\n","epochs = 40\n","\n","# Train dataset preparation\n","\n","all_data_record=load_tf_records(train_dataset_path).map(lambda x: decode_dataset(x, image_size), num_parallel_calls=AUTO,deterministic=False).cache().map(lambda x,y: apply_augmentations(x,y,image_size),num_parallel_calls=AUTO,deterministic=False)\n","\n","@tf.function()\n","def get_synthetic(ds):\n","  return ds.filter(lambda x, y: tf.equal(y['regressor'][-1],1.))\n","\n","@tf.function()\n","def get_lightbox(ds):\n","  return ds.filter(lambda x, y: tf.equal(y['regressor'][-1],2.))\n","\n","@tf.function()\n","def get_sunlamp(ds):\n","  return ds.filter(lambda x, y: tf.equal(y['regressor'][-1],3.))\n","\n","synthetic_ds = get_synthetic(all_data_record).repeat().take(15992*3) #few images repeated\n","lightbox_ds = get_lightbox(all_data_record).repeat().take(23988) # Repeat as many time as needed and take 15992*3/2\n","sunlamp_ds = get_sunlamp(all_data_record).repeat().take(23988) # Repeat as many time as needed and take 15992*3/2\n","\n","real_ds = lightbox_ds.concatenate(sunlamp_ds).shuffle(1000, seed=1) # Small shuffle buffer for performance reasons\n","\n","# Batch the datasets for correct interleaving ratio:\n","synthetic_ds_batched = synthetic_ds.batch(3)\n","real_ds_batched = real_ds.batch(3)\n","\n","# Populate batch 50-50 synthetic-real\n","train_ds=tf.data.Dataset.zip((synthetic_ds_batched, real_ds_batched)).map(lambda x, y: (tf.concat((x[0], y[0]), axis=0), {'discriminator': tf.concat((x[1]['discriminator'],y[1]['discriminator']),axis=0), 'regressor': tf.concat((x[1]['regressor'],y[1]['regressor']),axis=0)}),num_parallel_calls=AUTO).unbatch().prefetch(AUTO)\n","\n","train_dataset = train_ds.batch(batch_size,drop_remainder=True).repeat()\n","\n","# Validation dataset preparation\n","validation_dataset=load_tf_records(validation_dataset_path).map(lambda x: decode_dataset(x, image_size), num_parallel_calls=AUTO,deterministic=False).map(lambda x, y: map_validation_dataset(x,y, image_size), num_parallel_calls=AUTO).batch(batch_size,drop_remainder=True).cache().repeat().prefetch(AUTO)\n","\n","# Distribute datasets on TPU\n","train_ds_distributed=tpu_strategy.experimental_distribute_dataset(train_dataset)\n","valid_ds_distributed=tpu_strategy.experimental_distribute_dataset(validation_dataset)\n","\n","dataset_size = 15992*3*2\n","validation_size = 11994\n","\n","steps_per_epoch=dataset_size//batch_size\n","validation_steps=validation_size//batch_size\n","\n","# Create train dataset iterator (already batched)\n","train_iterator = iter(train_ds_distributed)\n","valid_iterator = iter(valid_ds_distributed)\n"],"metadata":{"id":"HmVyPK8cQFyR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define losses and metrics."],"metadata":{"id":"LQRZPa1k3uyZ"}},{"cell_type":"code","source":["GLOBAL_BATCH_SIZE = batch_size\n","synthetic_images_per_replica = 3 # With global batch size 48 and 6 batch size per replica\n","\n","### Losses:\n","with tpu_strategy.scope():\n","  # initialize cls loss with no reduction, reference: https://www.tensorflow.org/tutorials/distribute/custom_training#define_the_loss_function\n","  # from logits = True when we do not apply a sigmoid function to the output dense layer\n","  cls_lossobj = tf.keras.losses.BinaryCrossentropy(\n","      reduction=tf.keras.losses.Reduction.NONE,\n","      from_logits=True)\n","\n","  def discriminator_loss(labels, predictions):\n","    per_example_loss = cls_lossobj(labels, predictions)\n","    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=batch_size)\n","\n","  def mae_loss(y_true,y_pred):\n","    # We know that the first three elements of the batch (axis 0) are the synthetic images.\n","    #y_true also contains the dataset_class, used to interleave synthetic and real images\n","    y_true = y_true[0:3,0:4]\n","    y_pred = y_pred[0:3,0:4]\n","\n","    # Compute the absolute error for each coordinate and take the average\n","    per_example_loss = tf.math.reduce_mean(tf.math.abs(y_pred-y_true),axis=-1)\n","\n","    synthetic_images_per_batch = synthetic_images_per_replica*8\n","\n","    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=synthetic_images_per_batch)\n","\n","  def val_mae_loss(y_true,y_pred):\n","    \n","    # Compute the absolute error for each coordinate and take the average\n","    per_example_loss = tf.math.reduce_mean(tf.math.abs(y_pred-y_true),axis=-1)\n","\n","    # In the validation loss we average over the entire batch since all images are synthetic\n","    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n","\n","### Metrics\n","with tpu_strategy.scope(): \n","  \n","  # train loss accumulators\n","  encoder_loss_tracker = tf.keras.metrics.Mean(name=\"encoder_loss\")\n","  bbox_loss_tracker = tf.keras.metrics.Mean(name=\"bbox_loss\")\n","  cls_loss_tracker = tf.keras.metrics.Mean(name=\"cls_loss\")\n","\n","  # validation loss accumulators\n","  encoder_val_loss_tracker = tf.keras.metrics.Mean(name=\"encoder_val_loss\")\n","  bbox_val_loss_tracker = tf.keras.metrics.Mean(name=\"bbox_val_loss\")\n","  cls_val_loss_tracker = tf.keras.metrics.Mean(name=\"cls_val_loss\")\n","\n","  # Binary accuracy references: https://www.tensorflow.org/hub/tutorials/tf2_text_classification and https://github.com/tensorflow/tensorflow/issues/41413\n","\n","  # Threshold = 0 when from logits True in loss\n","  cls_accuracy_tracker = tf.keras.metrics.BinaryAccuracy(name='cls_accuracy', threshold=0.0)\n","  cls_val_accuracy_tracker = tf.keras.metrics.BinaryAccuracy(name='cls_val_accuracy', threshold=0.0)"],"metadata":{"id":"9gs_Ef-nNPK3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Optimizers and checkpoint manager."],"metadata":{"id":"YmOS68bF4PVQ"}},{"cell_type":"code","source":["import time\n","K = tf.keras.backend \n","import tensorflow_addons as tfa\n","\n","logger=tf.get_logger()\n","\n","total_steps = steps_per_epoch*epochs\n","\n","with tpu_strategy.scope(): \n","  \n","  # gamma value to be used for first epoch\n","  gamma = K.variable(0.0)\n","\n","  # We define an optimizer for each NN element\n","  optimizer_encoder=tfa.optimizers.AdamW(weight_decay=1e-8,\n","      learning_rate=tf.keras.optimizers.schedules.CosineDecay(5e-5, total_steps)\n","  )\n","  \n","  optimizer_regressor=tfa.optimizers.AdamW(weight_decay=1e-8,\n","      learning_rate=tf.keras.optimizers.schedules.CosineDecay(5e-5, total_steps)\n","  )\n","\n","  optimizer_discriminator=tfa.optimizers.AdamW(weight_decay=1e-8,\n","      learning_rate=tf.keras.optimizers.schedules.CosineDecay(5e-5, total_steps)\n","  )\n","\n","  checkpoint = tf.train.Checkpoint(\n","      epoch=tf.Variable(-1), # we add 1 as soon as we start training\n","      optimizer_encoder=optimizer_encoder,\n","      optimizer_regressor=optimizer_regressor,\n","      optimizer_discriminator=optimizer_discriminator,\n","      network=network,\n","      )\n","  \n","  manager = tf.train.CheckpointManager(checkpoint,checkpoint_prefix, max_to_keep=3)"],"metadata":{"id":"fMBEKkGbf3yg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Custom training loop: note, the code is configured for Swin Transformers backbones. To train the EfficientNet based model, modify the \"index\" variables in the cell below."],"metadata":{"id":"47GcVpWQ4eXX"}},{"cell_type":"code","source":["@tf.function\n","def train_step(iterator,steps_per_epoch):\n","  \"\"\"The step function for one training step.\"\"\"\n","  def step_fn(data):\n","        images, y_true = data\n","        y_truecls = tf.reshape(y_true['discriminator'],[GLOBAL_BATCH_SIZE//8,1])\n","\n","        with tf.GradientTape(persistent=True) as tape:\n","          # Forward pass\n","          y_pred_cls, y_pred_bbox = network(images,training=True)\n","\n","          # Compute losses \n","          cls_loss = discriminator_loss(y_truecls,y_pred_cls)\n","          bbox_loss=mae_loss(y_true['regressor'],y_pred_bbox)\n","          encoder_loss = bbox_loss - cls_loss*gamma*0.01 \n","\n","        # Accumulate loss, reference: https://www.tensorflow.org/guide/tpu#improving_performance_with_multiple_steps_inside_tffunction\n","        bbox_loss_tracker.update_state(bbox_loss*tpu_strategy.num_replicas_in_sync)\n","        cls_loss_tracker.update_state(cls_loss*tpu_strategy.num_replicas_in_sync)\n","        encoder_loss_tracker.update_state(encoder_loss*tpu_strategy.num_replicas_in_sync)\n","\n","        cls_accuracy_tracker.update_state(y_truecls , y_pred_cls)\n","\n","        # Compute gradients for encoder\n","        trainable_vars_encoder = network.get_layer(index=1).trainable_variables\n","        gradients_encoder = tape.gradient(encoder_loss, trainable_vars_encoder)\n","\n","\n","        # Update weights for encoder: https://www.tensorflow.org/guide/tpu#improving_performance_with_multiple_steps_inside_tffunction\n","        optimizer_encoder.apply_gradients(list(zip(gradients_encoder, trainable_vars_encoder))) # capire perchè in use tpu guide c'è list\n","\n","        # Compute gradients for discrimination head\n","        trainable_vars_discriminator = network.get_layer(index=3).trainable_variables # set index = 2 for EfficientNet\n","        gradients_discriminator = tape.gradient(cls_loss, trainable_vars_discriminator)\n","        \n","        # Update weights for discrimination head\n","        optimizer_discriminator.apply_gradients(list(zip(gradients_discriminator, trainable_vars_discriminator)))\n","\n","        # Compute gradients for regression head\n","        trainable_vars_regressor = network.get_layer(index=4).trainable_variables # set index = 3 for EfficientNet\n","        gradients_regressor = tape.gradient(bbox_loss, trainable_vars_regressor)\n","        \n","        # Update weights for regression head\n","        optimizer_regressor.apply_gradients(list(zip(gradients_regressor, trainable_vars_regressor)))\n","\n","\n","  for _ in tf.range(steps_per_epoch):\n","    tpu_strategy.run(step_fn, args=(next(iterator),))\n","\n","@tf.function\n","def valid_step(data_iter,validation_steps):\n","  def valid_step_fn(data):\n","        images, y_true = data\n","        y_truecls = tf.reshape(y_true['discriminator'],[GLOBAL_BATCH_SIZE//8,1])\n","\n","        y_pred_cls, y_pred_bbox = network(images,training=False)\n","        \n","        # Compute losses\n","        cls_val_loss = discriminator_loss(y_truecls,y_pred_cls)\n","        bbox_val_loss=val_mae_loss(y_true['regressor'],y_pred_bbox)\n","        encoder_val_loss = bbox_val_loss - cls_val_loss*gamma*0.01\n","\n","        # Accumulate losses\n","        bbox_val_loss_tracker.update_state(bbox_val_loss*tpu_strategy.num_replicas_in_sync)\n","        cls_val_loss_tracker.update_state(cls_val_loss*tpu_strategy.num_replicas_in_sync)\n","        encoder_val_loss_tracker.update_state(encoder_val_loss*tpu_strategy.num_replicas_in_sync)\n","\n","        # Compute and accumulate accuracy\n","        cls_val_accuracy_tracker.update_state(y_truecls , y_pred_cls)\n","        \n","\n","  for _ in tf.range(validation_steps):\n","    tpu_strategy.run(valid_step_fn, args=(next(data_iter),))"],"metadata":{"id":"jANvATBl4gSf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def restore_model(manager):\n","  with tpu_strategy.scope(): \n","    if manager.latest_checkpoint:\n","      checkpoint.restore(manager.latest_checkpoint)\n","      print(\"Restored from {}\".format(manager.latest_checkpoint))\n","    else:\n","      print(\"Initializing from scratch.\")\n","    return checkpoint\n","\n","def train_model(eventually_restored_checkpoint,manager):    \n","    \n","  while checkpoint.epoch.numpy() +1 < epochs:\n","    epoch_start_time = time.time()\n","    eventually_restored_checkpoint.epoch.assign_add(1)\n","    epoch=eventually_restored_checkpoint.epoch.numpy()\n","    print('Epoch: {}, initial learning rate: {}'.format(epoch+1, round(optimizer_encoder._decayed_lr(tf.float32).numpy(),10)))\n","    \n","    # update gamma\n","    K.set_value(gamma, 2.0/(1.0+tf.math.exp(-10.0*(epoch+1)/epochs))-1.0) \n","\n","    #train step\n","    train_step(train_iterator,steps_per_epoch)\n","\n","    print('Current step: {}, encoder loss: {}, bbox loss: {}, cls loss: {}, cls_accuracy: {}\\n'.format(\n","      optimizer_encoder.iterations.numpy(),\n","      round(float(encoder_loss_tracker.result()), 4),\n","      round(float(bbox_loss_tracker.result()), 4),\n","      round(float(cls_loss_tracker.result()), 4),\n","      round(float(cls_accuracy_tracker.result()),4)))\n","\n","    # validation step\n","    valid_step(valid_iterator,tf.convert_to_tensor(validation_steps))\n","    print('Validation data - encoder loss: {}, bbox loss: {}, cls loss: {}, cls_accuracy: {}\\n'.format(\n","      round(float(encoder_val_loss_tracker.result()), 4),\n","      round(float(bbox_val_loss_tracker.result()), 4),\n","      round(float(cls_val_loss_tracker.result()), 4),\n","      round(float(cls_val_accuracy_tracker.result()),4)))\n","\n","    # Export checkpoints every 5 epochs \n","    if (epoch+1) % 5 == 0:\n","      print('\\n Saving checkpoint...\\n')\n","\n","      #questo scope forse non serve\n","      with tpu_strategy.scope():\n","        manager.save()\n","\n","    # Train accumulators reset\n","    encoder_loss_tracker.reset_states()\n","    bbox_loss_tracker.reset_states()\n","    cls_loss_tracker.reset_states()\n","    cls_accuracy_tracker.reset_states()\n","\n","    # Validation accumulators reset\n","    encoder_val_loss_tracker.reset_states()\n","    bbox_val_loss_tracker.reset_states()\n","    cls_val_loss_tracker.reset_states()\n","    cls_val_accuracy_tracker.reset_states()"],"metadata":{"id":"DdG-kkYrXIgn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Resume checkpoint."],"metadata":{"id":"orWRMv865koX"}},{"cell_type":"code","source":["### TO RESTORE FROM CHECKPOINT RUN THIS CELL BEFORE TRAINING\n","\n","del(network,optimizer_regressor,optimizer_encoder,optimizer_discriminator)\n","\n","checkpoint=restore_model(manager)\n","network=checkpoint.network\n","optimizer_regressor=checkpoint.optimizer_regressor\n","optimizer_encoder=checkpoint.optimizer_encoder\n","optimizer_discriminator=checkpoint.optimizer_discriminator"],"metadata":{"id":"W7UMIwySqHo7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Start training and export weights."],"metadata":{"id":"dD1pG-8H5nBf"}},{"cell_type":"code","source":["train_model(checkpoint,manager)\n","network.save_weights(weights_export_dir)"],"metadata":{"id":"IUUAAGccYfDd"},"execution_count":null,"outputs":[]}]}